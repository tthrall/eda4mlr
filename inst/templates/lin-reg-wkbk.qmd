---
title: "Chapter 7 Workbook: Linear Regression"
subtitle: "EDA for Machine Learning"
author: "Your Name"
date: today
format:
  html:
    toc: true
    embed-resources: true
    df-print: paged
execute:
  warning: false
  message: false
---

```{r}
#| label: setup
#| include: false

library(ggplot2)
library(dplyr)
library(eda4mlr)

```

## Learning Objectives

Here are the learning goals for this chapter:

1. In a scatter diagram, identify whether a given point represents an observation or a feature.
2. In the standard formulation of a linear regression model, identify whether a column of the X matrix represents an observation or a feature.
3. Describe how the concept of orthogonal projection applies to linear least-squares regression.
4. In linear regression, define the normal equations.
5. In linear regression, define the orthogonal projection matrix derived from the normal equations.
6. Explain why centering data simplifies the geometry of linear regression.
7. In linear regression, explain why residuals are orthogonal to all feature vectors.
8. Describe covariance and correlation as inner products of centered vectors.

## The Big Picture

> What does it mean geometrically to "fit" a line to data—and why does linear algebra provide the natural language for regression?

---

## Choosing Your Dataset

For this workbook, you'll need a dataset with:

- At least 30 observations
- At least 2 numeric variables (one response, one or more predictors)
- Something you're genuinely interested in

**Suggested sources:**

- `datasets::mtcars` — mpg vs. weight, horsepower, displacement
- `datasets::swiss` — fertility vs. education, agriculture, etc.
- `MASS::Boston` — median home value vs. various predictors

**Fallback option:** `datasets::cars` provides a simple speed vs. stopping distance example.

```{r}
#| label: load-data

# Load your dataset here
# Example: my_data <- datasets::mtcars

```

---

## Exercise 1: The Geometry of Data

### Task 1.1: Observations vs. Features

Create a scatter plot of two numeric variables. Then answer:

- In the scatter plot, does each point represent an observation or a feature?
- If you arranged your data as a matrix X with observations in rows, what would the columns represent?

```{r}
#| label: ex1-scatter

# Your code here

```

*Your answers:*



### Task 1.2: The Design Matrix

Construct the design matrix X for a simple linear regression (include a column of 1s for the intercept). Verify the dimensions.

```{r}
#| label: ex1-design-matrix

# Your code here

```

*How many rows? How many columns? What does each represent?*



---

## Exercise 2: The Normal Equations

### Task 2.1: Solving the Normal Equations

The normal equations are: $(X^T X) \hat{\beta} = X^T y$

Solve for $\hat{\beta}$ manually using matrix operations, then verify your answer matches `lm()`.

```{r}
#| label: ex2-normal-equations

# Your code here

```

*Do your coefficients match?*



### Task 2.2: The Projection Matrix

The projection matrix (or "hat matrix") is: $H = X(X^T X)^{-1} X^T$

Compute H and verify that the fitted values equal $Hy$.

```{r}
#| label: ex2-projection

# Your code here

```

*What are the dimensions of H? What does H do geometrically?*



---

## Exercise 3: Centering and Orthogonality

### Task 3.1: Why Center?

Fit a regression with and without centering your predictor variables. Compare:

- The estimated slopes
- The estimated intercept
- The interpretation of each

```{r}
#| label: ex3-centering

# Your code here

```

*How does centering change the intercept? Does it change the slopes?*



### Task 3.2: Residuals Are Orthogonal

Compute the residuals from your regression. Verify that:

1. The residuals are orthogonal to the vector of 1s (i.e., they sum to zero)
2. The residuals are orthogonal to each predictor vector

```{r}
#| label: ex3-orthogonality

# Your code here

```

*Why must this be true geometrically?*



---

## Exercise 4: Covariance as Inner Product

### Task 4.1: Inner Products of Centered Vectors

For two centered vectors $x$ and $y$, the sample covariance is proportional to their inner product:

$$\text{Cov}(x, y) = \frac{1}{n-1} x^T y$$

Verify this relationship using your data.

```{r}
#| label: ex4-covariance

# Your code here

```

### Task 4.2: Correlation as Cosine

Show that the correlation coefficient equals the cosine of the angle between centered vectors:

$$r = \frac{x^T y}{\|x\| \|y\|}$$

Compute this directly and compare to `cor()`.

```{r}
#| label: ex4-correlation-cosine

# Your code here

```

*What does it mean geometrically when r = 1? When r = 0?*



---

## Practice Problems

### Problem 1

In multiple regression, the coefficient for a predictor can change sign when other predictors are added. Demonstrate this with an example and explain geometrically why it happens.

```{r}
#| label: practice-1

# Your code here

```

*Your explanation:*



### Problem 2

Explain in your own words why the residuals must be orthogonal to the column space of X. What would it mean if they weren't?

*Your explanation:*


