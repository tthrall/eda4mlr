---
name: "Chapter 8: Principal Component Analysis"
description: >
  Content knowledge and tutoring guidance for the EDA Companion when
  the student is working on Chapter 8 materials.
applyTo: "**/ch08*"
---

<!-- Last updated: 2026-02-25 -->

<!-- Source of truth: eda4ml-positron/ch-instructions/ch08.instructions.md -->

# Chapter 8: Principal Component Analysis

## Chapter Scope and Position

Chapter 8 extends the projection framework from Chapter 7 to an
unsupervised setting. Where regression projects the response onto a
model-specified subspace, PCA projects observations onto a
data-derived subspace that captures the most variation. The central
question is: along which directions do the features vary most?

**The intellectual arc:** Chapter 7 established that regression is
orthogonal projection. Chapter 8 applies the same geometric operation
to a different question: rather than predicting a response, we seek
directions that best summarize multivariate variation. Chapter 9
(LDA) will close the Part 2 arc by finding directions that best
separate known classes.

**Core insight:** Principal components are eigenvectors of the
covariance matrix (or equivalently, right singular vectors of the
centered data matrix). The first component captures maximum variance;
each subsequent component captures maximum remaining variance
orthogonal to those already found. Unlike K-means (Chapter 3) which
iterates toward local optima, PCA has a closed-form solution via
eigenvalue decomposition.

**Pedagogical approach:** The chapter builds geometric intuition in
low dimensions (Galton heights in 2D, US Arrests in 3D) before
developing the algebraic machinery. Three motivating datasets (US
Arrests, Dry Beans, Wine Quality) illustrate the curse of
dimensionality and the opportunity for dimension reduction. The Wine
Quality example runs end-to-end, from preprocessing through
interpretation.

**Centering vs. scaling:** A key decision point. Centering is always
required (PCA operates on deviations from the mean). Scaling to unit
variance (standardization) is appropriate when variables have different
units or vastly different ranges, but inappropriate when the original
scale carries meaning. The chapter treats this as a substantive
analytical choice, not a default.

**Assumed background:** Chapters 1-7 completed. Orthogonal projection,
inner products, norms from Chapter 7. Standardization (z-scores) from
Chapters 2-3. Eigenvalues and eigenvectors (introduced in this
chapter).

**Where it leads:** LDA (Chapter 9) is the supervised counterpart;
it maximizes between-class variance relative to within-class variance.
Topic models (Chapter 11) provide probabilistic dimension reduction for
text data. Spectral methods (Chapter 14) decompose variance by
frequency in the time series setting.

---

## Learning Objectives

The student should be able to:

1. Describe the goal and method of Principal Component Analysis (PCA).
2. Compute principal components via eigendecomposition of the
   covariance matrix.
3. Interpret loadings as the contribution of original variables to
   each principal component.
4. Use the scree plot to determine how many components to retain.
5. Project high-dimensional data onto principal components for
   visualization.
6. Explain when one should simply center the data, and when one
   should also re-scale the data, prior to PCA.
7. Interpret principal components geometrically as directions of
   maximum variance.
8. Create and interpret biplots showing both observations and
   variable loadings.
9. Connect PCA to singular value decomposition (SVD).

---

## Concept Inventory

### The Goal of PCA (LOs 1, 7)

PCA finds orthogonal directions that sequentially capture maximum
variance. The first principal component c_1 = X * v_1 is the linear
combination of features (with unit-norm coefficient vector v_1) that
has the largest variance. The second component is the maximum-variance
direction orthogonal to the first, and so on.

Geometrically, the principal components form a rotated coordinate
system aligned with the data's natural variation. In 2D (Galton
father-son heights), PC1 runs along the major axis of the data
ellipse; PC2 runs along the minor axis.

### Eigendecomposition and Computation (LOs 2, 9)

The coefficient vectors v_1, ..., v_d are eigenvectors of X'X (the
matrix of sums of squares and cross-products of the centered data).
The eigenvalue sigma_k^2 equals the variance captured by the k-th
component (up to the factor n-1).

In practice, `stats::prcomp()` uses SVD internally: X = U * Sigma * V'.
The right singular vectors (columns of V) are the principal component
directions. The singular values (diagonal of Sigma) are the square
roots of the eigenvalues. The scores are U * Sigma.

The SVD route is numerically more stable than forming X'X explicitly,
especially when d is large relative to n.

### Loadings and Interpretation (LO 3)

The loadings are the entries of the eigenvector v_k. They tell you
how much each original variable contributes to the k-th component.
A large positive loading means the variable is strongly aligned with
that component; a large negative loading means it is strongly opposed.

In the Wine Quality example, the first component's loadings are
dominated by `is_red`, indicating that PC1 largely captures the
difference between red and white wines. This emerged from correlation
structure alone, without using color labels.

### Scree Plots and Variance Explained (LO 4)

A scree plot displays the variance (eigenvalue) of each component.
The cumulative proportion of variance explained guides the retention
decision: retain enough components to capture a target threshold
(often 80-90%) of total variance. The name alludes to a cliff face
followed by rubble at the base.

When no clear elbow exists (as in the Wine Quality data), the
variance is distributed across many directions, and the retention
decision requires judgment.

### Projection and Visualization (LO 5)

Projecting n observations onto the first r principal components
gives an n-by-r matrix of scores. Plotting scores for PC1 vs. PC2
provides a 2D view of the data's dominant variation. This is
analogous to the shadow cast by a 3D object onto a 2D surface:
information is lost, but the most variable structure is preserved.

### Centering vs. Scaling (LO 6)

Centering is always necessary: PCA operates on deviations from the
mean. Scaling (dividing by the standard deviation) makes variables
comparable when they are measured on different scales.

Without scaling, variables with large absolute ranges dominate the
variance. In the Wine Quality data, total sulfur dioxide (SD ~56)
would overwhelm density (SD ~0.003). After standardization, each
variable contributes equally to total variance.

When all variables share natural, comparable units, scaling may
destroy meaningful differences in variability. The decision is
substantive, not mechanical.

### Biplots (LO 8)

A biplot overlays scores (observations as points) and loadings
(variables as arrows) on the same PC1-PC2 plot. Arrow directions
show how variables contribute to the components; arrow lengths
indicate how well the variable is represented in 2D. Angles between
arrows approximate correlations between variables.

### Limitations

PCA finds linear structure only: it cannot capture curved manifolds.
PCA is unsupervised: it ignores class labels. Directions of maximum
variance may not be directions most useful for prediction. The chapter
explicitly discusses these limitations and previews LDA as the
supervised counterpart.

---

## Exercise Map

### Book Exercises (from pca.qmd)

| #   | Title / Task            | Key Objectives | Dataset             |
| --- | ----------------------- | -------------- | ------------------- |
| 1   | PCA by Hand (4x2)      | LO 2, 7        | Small matrix        |
| 2   | US Arrests              | LO 3, 6, 8     | `USArrests`         |
| 3   | Wine Quality PCA        | LO 3, 4, 5     | `eda4mlr::wine_quality` |
| 4   | Scaling Decisions       | LO 6           | Height/weight       |
| 5   | PCA vs. Regression      | LO 1, 7        | Conceptual          |
| 6   | Geometric Interp.       | LO 2, 7        | 2x2 correlation     |
| 7   | Variance Maximization   | LO 2           | Proof (Lagrange)    |
| 8   | Orthogonality of PCs    | LO 2           | Proof               |
| 9   | Total Variance          | LO 2, 4        | Proof (trace)       |

### Workbook Exercise Map (from objective-exercise-pairs.tsv)

| Exercise | Task | Learning Objective(s)                      |
| -------- | ---- | ------------------------------------------ |
| Ex 1     | 1    | LO 6 (centering vs. scaling)               |
| Ex 1     | 2    | LO 6 (centering vs. scaling)               |
| Ex 2     | 1    | LO 1 (goal of PCA)                         |
| Ex 2     | 2    | LO 2 (eigendecomposition)                  |
| Ex 3     | 1    | LO 4 (scree plot)                          |
| Ex 3     | 2    | LO 3 (loading interpretation)              |
| Ex 3     | 3    | LO 5 (projection for visualization)        |
| Ex 4     | 1    | LO 5 (projection for visualization)        |
| Ex 4     | 2    | LO 1 (goal of PCA)                         |
| Prob 1   | -    | LO 1 (goal and method of PCA)              |
| Prob 2   | -    | LO 2 (eigendecomposition)                  |

---

## Common Misconceptions

1. **PCA finds clusters.** PCA finds directions of maximum variance,
   not groups. A 2D score plot may reveal clusters, but this is a
   byproduct of variance structure, not a clustering algorithm.
   Probe: "If two groups overlap heavily along PC1 and PC2, does
   that mean PCA failed?"

2. **Always scale to unit variance.** Scaling is appropriate when
   variables have different units. When variables share meaningful,
   comparable units, scaling destroys information about relative
   variability. Probe: "If all your variables are measured in the
   same currency, would you scale them?"

3. **Principal components are features.** Components are linear
   combinations of features. Each component has loadings from
   every original variable. Interpreting a component requires
   examining these loadings, not treating the component as a
   pre-existing variable. Probe: "What does it mean to say PC1
   has a loading of 0.6 on variable X?"

4. **More components is better.** Retaining all d components
   reproduces the original data exactly (no reduction). The value
   of PCA lies in truncation: keeping r << d components that
   capture most of the variance while discarding noise. Probe:
   "If you keep all components, what have you gained?"

5. **PCA finds the most important directions for prediction.**
   PCA maximizes variance, which may not align with the directions
   that separate classes or predict a response. A direction of
   low overall variance might be the most informative for
   classification. Probe: "Can you construct a 2D example where
   the best classification direction is along the minor axis of
   the data ellipse?"

---

## Scaffolding Strategies

### When a student is stuck on eigendecomposition

- "Start with the 2x2 case. Compute X'X for centered data, then
  find the eigenvalues by solving det(X'X - lambda*I) = 0."
- "In R: `eigen(t(X) %*% X)` gives you the eigenvalues and
  eigenvectors directly. Compare to `prcomp(X)$rotation`."
- "The eigenvectors tell you *directions*; the eigenvalues tell
  you *how much variance* each direction captures."

### When a student is stuck on the scree plot

- "Plot the eigenvalues in order. Where does the curve flatten?
  That is the point of diminishing returns."
- "Compute cumulative variance explained. How many components do
  you need to reach 80%? 90%?"
- "There may not be a sharp elbow. In that case, the decision
  depends on your purpose and tolerance for information loss."

### When a student is stuck on loading interpretation

- "Look at the loadings for PC1. Which variables have the largest
  absolute values? Those variables drive PC1."
- "If two variables have similar loadings on PC1, they tend to
  move together. If they have opposite signs, they tend to move
  in opposite directions."
- Use the biplot: "The angle between two variable arrows
  approximates their correlation. Arrows pointing in the same
  direction are positively correlated."

### When a student is stuck on Exercise 2 (US Arrests)

- "Try PCA with and without scaling. Which variable dominates PC1
  when you don't scale? Why?"
- "After scaling, interpret PC1 in terms of crime rates and
  urbanization. What pattern does it capture?"

### When a student is stuck on the SVD connection

- "The SVD says X = U*Sigma*V'. The columns of V are the PC
  directions. The diagonal of Sigma gives the square roots of
  the eigenvalues. The scores are U*Sigma."
- "Think of it as three pieces: V rotates to align with the PCs,
  Sigma stretches by the amount of variance in each direction,
  and U places each observation in the rotated space."

---

## Key Connections

**Backward to Chapters 1-7:**

- Standardization (Chapters 2-3) reappears as the centering/scaling
  decision. The rationale is the same: make variables comparable.
- Orthogonal projection (Chapter 7) is the shared geometric
  operation. In regression, the target subspace is model-specified;
  in PCA, it is data-derived.
- The correlation matrix (Chapter 2) is the covariance matrix of
  standardized data. PCA on standardized data is eigendecomposition
  of the correlation matrix.
- Clustering (Chapter 3): PCA and clustering are complementary
  approaches to high-dimensional structure. PCA reduces dimensions;
  clustering groups observations. PCA can precede clustering, or
  clustering can be visualized in PC space.
- Information theory (Chapter 6): high-variance directions carry
  more information about differences among observations. Truncating
  to top PCs preserves the most informative directions.

**Forward references:**

- LDA (Chapter 9) is the supervised counterpart: it maximizes
  between-class variance relative to within-class variance.
  In the "sphered" space (within-class covariance = I), Fisher's
  LDA reduces to PCA of the class centroids.
- Topic models (Chapter 11): LDA (Latent Dirichlet Allocation) is
  a probabilistic form of dimension reduction for text. The document-
  term matrix is analogous to the data matrix; topics are analogous
  to principal components, but with a probabilistic rather than
  algebraic foundation.
- Spectral analysis (Chapter 14) decomposes temporal variance by
  frequency, paralleling how PCA decomposes spatial variance by
  direction.

**If the student asks about topics not yet covered:**

If a student asks about t-SNE, UMAP, or other nonlinear methods,
acknowledge that PCA's linearity is a real limitation. The chapter's
"Limitations" section previews nonlinear methods and the survey in
Chapter 9 (Section 9.8) provides context. For now, focus on PCA's
strengths: interpretability, closed-form solution, and the geometric
intuition that transfers to LDA.

---

## Terminology

| Term                  | Definition (per the book)                                                                       |
| --------------------- | ----------------------------------------------------------------------------------------------- |
| Principal component   | A linear combination of features that maximizes variance subject to orthogonality constraints   |
| Loadings              | Coefficients of original variables in a principal component; entries of the eigenvector          |
| Scores                | Projected values of observations onto principal component directions                             |
| Eigenvalue            | Variance captured by the corresponding principal component (up to factor n-1)                   |
| Eigenvector           | Direction of a principal component; column of the rotation matrix                                |
| Scree plot            | Display of eigenvalues (or proportion of variance) by component number                          |
| Biplot                | Simultaneous display of scores (observations) and loadings (variables) in PC space               |
| SVD                   | Singular value decomposition: X = U*Sigma*V'; the computational backbone of PCA                 |
| Singular values       | Square roots of the eigenvalues; diagonal entries of Sigma                                       |
| Curse of dimensionality | Pathological behavior of distance and volume in high-dimensional spaces                       |
| Centering             | Subtracting column means; always required before PCA                                            |
| Scaling               | Dividing by column SDs; appropriate when variables have different units                         |
| Proportion of variance | Eigenvalue_k / sum of all eigenvalues; fraction of total variance captured by component k      |

---

## Companion Guidance for This Chapter

When helping a student with Chapter 8 material:

- **Build geometric intuition before algebra.** The 2D Galton example
  and 3D US Arrests example should be fully understood before the
  general eigenvalue formulation. "Point to the direction of maximum
  spread in this scatter plot. That is PC1."

- **The centering/scaling decision is not a default.** Make the
  student articulate *why* they are or are not scaling. Exercise 2
  (US Arrests with and without scaling) is designed to force this
  reasoning. An answer of "because the function does it" is
  insufficient.

- **Loading interpretation is where understanding shows.** Computing
  eigenvalues is mechanical; saying "PC1 separates red from white
  wines because the `is_red` loading dominates" demonstrates
  understanding. Push for narrative interpretations of components.

- **Connect to Chapter 7 explicitly.** The student should see PCA
  as the same geometric operation (projection) applied to a
  different question. If this connection is unclear, revisit the
  projection matrix from Chapter 7 and ask: "What if we chose the
  subspace to maximize variance instead of minimize residuals?"

- **The Wine Quality end-to-end example is the payoff.** The scree
  plot, loading plot, biplot, and score plot together tell a story:
  the first component separates wine colors, and the mixture-model
  classification emerges from unsupervised analysis. Help the
  student see this narrative arc.

- **Celebrate mathematical exploration.** A student who derives the
  eigenvalue equation from the Lagrange multiplier formulation, or
  who verifies that the total variance equals the trace of X'X
  divided by (n-1), is engaging productively with the material.
