---
name: "Chapter 7: Linear Regression"
description: >
  Content knowledge and tutoring guidance for the EDA Companion when
  the student is working on Chapter 7 materials.
applyTo: "**/ch07*"
---

<!-- Last updated: 2026-02-25 -->

<!-- Source of truth: eda4ml-positron/ch-instructions/ch07.instructions.md -->

# Chapter 7: Linear Regression

## Chapter Scope and Position

Chapter 7 opens Part 2 (Linear Algebra Methods) by presenting linear
regression from a geometric perspective. The chapter does not teach
regression as a modeling recipe; the student already knows how to call
`lm()`. Instead, it reveals that least-squares regression *is*
orthogonal projection onto feature space, connecting familiar formulas
to the geometry of vectors, norms, and inner products.

**The intellectual arc:** Chapters 1-6 built the student's EDA
vocabulary (visualization, conditional distributions, clustering,
simulation, sampling, information theory). Chapter 7 shifts to the
language of linear algebra and establishes the geometric framework
that unifies regression, PCA (Chapter 8), and LDA (Chapter 9).

**Core insight:** Least-squares fitted values are the orthogonal
projection of the response vector onto the column space of the feature
matrix. The residual vector is orthogonal to every feature vector. This
is not an analogy; it is exactly what the normal equations compute.

**Row versus column perspectives:** The chapter develops two
complementary views of a data matrix. In the row perspective,
observations are points in d-dimensional space and regression fits a
surface through the point cloud. In the column perspective, features
are vectors in n-dimensional space and regression projects the response
onto the subspace they span. Most visualizations use the row view;
the algebra is most naturally expressed in the column view.

**Pedagogical approach:** The chapter builds from Galton's heights
(simple regression, 1 predictor) through the OECD Better Life Index
(many predictors) to the general matrix formulation. Centering is
introduced as a geometric simplification that eliminates the intercept.
The Pythagorean decomposition connects the algebra to the familiar
R-squared statistic.

**Assumed background:** Chapters 1-6 completed. Scatter plots,
correlation, z-scores from Chapter 2. Basic matrix operations
(transpose, multiplication, inverse). The chapter introduces norms and
inner products from scratch.

**Where it leads:** PCA (Chapter 8) uses the same projection framework
but onto data-derived subspaces rather than model-specified ones. LDA
(Chapter 9) projects onto subspaces that maximize class separation.
Regularization (ridge, LASSO) modifies the projection to prevent
overfitting.

---

## Learning Objectives

The student should be able to:

1. In a scatter diagram, identify whether a given point represents an
   observation or a feature.
2. In the standard formulation of a linear regression model, identify
   whether a column of the X matrix represents an observation or a
   feature.
3. Describe how the concept of orthogonal projection applies to
   linear least-squares regression.
4. In linear regression, define the normal equations.
5. In linear regression, define the orthogonal projection matrix
   derived from the normal equations.
6. Explain why centering data simplifies the geometry of linear
   regression.
7. In linear regression, explain why residuals are orthogonal to all
   feature vectors.
8. Describe covariance and correlation as inner products of centered
   vectors.
9. Apply the Pythagorean decomposition to relate total, explained,
   and residual variance.

---

## Concept Inventory

### Row vs. Column Perspectives (LOs 1, 2)

In the row perspective, each observation is a point in R^d. A scatter
plot displays these points; the regression surface passes through the
cloud. In the column perspective, each feature is a vector in R^n
(one component per observation). The response y is also a vector in
this space, and regression projects y onto col(X).

The two perspectives describe the same mathematical object. The row
view is natural for visualization; the column view is natural for
algebra. Students trained in statistics typically think row-first;
the chapter deliberately develops the column perspective because it
makes the projection interpretation clear.

LOs 1 and 2 target the same distinction from different entry points:
LO 1 from a scatter plot, LO 2 from the design matrix.

### Orthogonal Projection (LOs 3, 5, 7)

The fitted values y-hat = X * beta-hat are the point in col(X)
closest to y. The residual vector epsilon = y - y-hat is perpendicular
to every vector in col(X). These two conditions (y-hat in col(X);
epsilon perpendicular to col(X)) define orthogonal projection.

The projection matrix P = X(X'X)^{-1}X' maps any response vector to
its least-squares prediction. P is symmetric (P' = P) and idempotent
(P^2 = P). Idempotence means projecting twice gives the same result
as projecting once.

The complementary projection (I - P) maps y to the residuals. Since
P and (I - P) project onto orthogonal subspaces, the fitted values
and residuals are orthogonal vectors: y-hat' * epsilon = 0.

### The Normal Equations (LO 4)

The orthogonality condition X'(y - X*beta-hat) = 0 rearranges to
X'X * beta-hat = X'y. These are the normal equations. When X'X is
invertible (linearly independent columns), the solution is
beta-hat = (X'X)^{-1} X'y.

The name "normal" refers to orthogonality (the residual is normal to
col(X)), not to the Gaussian distribution.

### Centering and the Centering Matrix (LO 6)

Centering each variable (subtracting its mean) shifts the origin to
the data centroid. After centering, the regression surface passes
through the origin, eliminating the intercept term. The slope
coefficients are unchanged.

The centering matrix C = I - (1/n) * 1*1' is itself a projection
matrix (symmetric, idempotent). It projects onto the subspace
orthogonal to the constant vector. Centered vectors satisfy
dot(v) perpendicular to 1.

### Covariance and Correlation as Inner Products (LO 8)

Sample covariance is (1/(n-1)) times the inner product of centered
vectors: cov(v, w) = dot_v' * dot_w / (n-1). Sample variance is the
squared norm: var(v) = ||dot_v||^2 / (n-1). Correlation is the cosine
of the angle between centered vectors.

This interpretation explains why regression coefficients can change
when predictors are added or removed: in non-orthogonal bases, the
projection of y onto one feature depends on the other features present.

### The Pythagorean Decomposition and R-squared (LO 9)

For centered data: ||dot_y||^2 = ||dot_y-hat||^2 + ||dot_epsilon||^2.
This is TSS = ESS + RSS. R-squared = ESS/TSS = cos^2(theta), where
theta is the angle between the centered response and the centered
fitted values.

### Multicollinearity

When feature vectors are nearly parallel (highly correlated), X'X is
nearly singular and small data perturbations cause large changes in
beta-hat. The chapter demonstrates this with the Galton data (moderate
correlation between mother's and father's heights) and notes that the
OECD data (many correlated indicators) is prone to overfitting.

### ML Connections

The chapter previews regularization (ridge regression adds lambda*I
to stabilize X'X), PCA (data-derived subspaces), LDA (class-separation
subspaces), kernel methods, and neural networks (linear transformations
plus nonlinear activations). These are described conceptually, not
developed.

---

## Exercise Map

### Book Exercises (from lin-reg.qmd)

| #   | Title / Section       | Key Objectives | Notes                                      |
| --- | --------------------- | -------------- | ------------------------------------------ |
| 1   | Row vs Column         | LO 1, 2        | Conceptual: identify observations/features |
| 2   | Normal Equations      | LO 4, 5        | Solve normal equations by hand             |
| 3   | Centering Heights     | LO 6, 7        | Program centered regression; compare       |
| 4   | Covariance as IP      | LO 8           | Inner-product notation for cov/cor         |

### Workbook Exercise Map (from objective-exercise-pairs.tsv)

| Exercise | Task | Learning Objective(s)                |
| -------- | ---- | ------------------------------------ |
| Ex 1     | 1    | LO 1, 2 (row/column identification) |
| Ex 1     | 2    | LO 2 (design matrix columns)        |
| Ex 2     | 1    | LO 4 (normal equations)              |
| Ex 2     | 2    | LO 5 (projection matrix)            |
| Ex 3     | 1    | LO 6 (centering)                    |
| Ex 3     | 2    | LO 7 (residual orthogonality)       |
| Ex 4     | 1    | LO 8 (covariance as inner product)  |
| Ex 4     | 2    | LO 8 (correlation as cosine)        |
| Prob 1   | -    | LO 3 (projection interpretation)    |
| Prob 2   | -    | LO 7 (residual orthogonality)       |

---

## Common Misconceptions

1. **Regression coefficients are just slopes.** In the column
   perspective, coefficients are coordinates of y-hat in the
   (generally non-orthogonal) basis formed by the feature vectors.
   When features are correlated, each coefficient depends on which
   other features are in the model. Probe: "If you add a new
   predictor to the model, why might the coefficient of an existing
   predictor change?"

2. **The normal equations have nothing to do with geometry.** The
   name "normal" refers to perpendicularity (the residual is normal
   to the column space), not to Gaussian distributions. The normal
   equations are the algebraic expression of the orthogonality
   condition. Probe: "What geometric condition does the equation
   X'(y - X*beta-hat) = 0 express?"

3. **Centering changes the model.** Centering eliminates the
   intercept without changing slope coefficients or predictions.
   It simplifies the algebra by making col(X) a proper subspace
   through the origin. Probe: "After centering, why don't we need
   a column of ones in X?"

4. **R-squared measures how good a model is.** R-squared measures
   the proportion of variance explained, but adding any predictor
   can only increase it. The Pythagorean decomposition shows why:
   projecting onto a larger subspace can only reduce ||epsilon||.
   Probe: "What happens to R-squared if you add a random noise
   variable as a predictor?"

5. **The projection matrix is impractical.** While P is n-by-n and
   impractical for large datasets, its conceptual role is central:
   it encapsulates the entire regression operation. Understanding
   P clarifies why fitted values are in col(X), why residuals are
   orthogonal, and why projecting twice is the same as projecting
   once. Probe: "What does it mean that P^2 = P?"

---

## Scaffolding Strategies

### When a student is stuck on row vs. column perspectives

- "In the scatter plot, each dot is one family. How many dimensions
  does that space have? Now think of father-heights as a vector with
  one entry per family. How many dimensions does *that* space have?"
- "The scatter plot is the row view. The algebra uses the column view.
  Both describe the same data. Which view shows the regression line?
  Which view shows the projection?"

### When a student is stuck on the normal equations

- "Start from the orthogonality condition: the residual must be
  perpendicular to every column of X. Write that as X'*epsilon = 0.
  Now substitute epsilon = y - X*beta-hat and rearrange."
- For the hand-calculation exercises: "Compute X'X and X'y first.
  These are small matrices you can multiply by hand."

### When a student is stuck on centering

- "Fit the model with and without centering. Compare the slope
  coefficients. Compare the fitted values. What changed and what
  stayed the same?"
- "After centering, the mean of each variable is zero. What does
  that tell you about the intercept?"

### When a student is stuck on the Pythagorean decomposition

- "Draw a right triangle in n-dimensional space with sides
  dot_y-hat and dot_epsilon. The hypotenuse is dot_y. The
  Pythagorean theorem applies because the sides are orthogonal."
- "Compute TSS, ESS, and RSS numerically. Verify that TSS = ESS + RSS.
  Now compute R-squared as ESS/TSS."

### When a student is stuck on Exercise 4 (covariance as inner product)

- "Write out the definition of sample covariance. Now compare it to
  the formula for the inner product of centered vectors. What
  constant differs?"
- "If covariance is an inner product, what does variance correspond
  to? What does correlation correspond to?"

---

## Key Connections

**Backward to Chapters 1-6:**

- The regression line from Chapter 2 reappears, now understood as
  a projection. The slope coefficient beta = r * sy/sx corresponds
  to the formula beta-hat = x'y / x'x for centered, standardized data.
- Z-scores (Chapter 2) and standardization (Chapter 3) connect to the
  inner-product interpretation of correlation.
- The Galton heights data from Chapter 2 is the running example,
  providing continuity.
- Simulation (Chapter 4) could generate data to verify projection
  properties empirically.
- Loss functions: WCSS from Chapter 3 is a special case of the
  general principle of minimizing squared distances. RSS in
  regression is the analogous quantity.

**Forward references:**

- PCA (Chapter 8) uses the same geometric machinery but the
  target subspace is data-derived (directions of maximum variance)
  rather than model-specified.
- LDA (Chapter 9) projects onto directions that maximize class
  separation, using within-class and between-class covariance.
- Regularization (ridge, LASSO) modifies the projection to handle
  multicollinearity and overfitting.
- The Pythagorean decomposition (TSS = ESS + RSS) recurs in ANOVA
  and in understanding explained variance throughout statistics.

**If the student asks about topics not yet covered:**

If a student asks about ridge regression, LASSO, or other
regularization methods, connect to the multicollinearity section:
when X'X is nearly singular, adding lambda*I makes it invertible
and shrinks coefficients. The chapter previews these ideas without
developing them. If a student asks about PCA, note that it involves
the same projection framework applied to a different question:
"what directions capture the most variance?" rather than "what
linear combination best predicts y?"

---

## Terminology

| Term                   | Definition (per the book)                                                                        |
| ---------------------- | ------------------------------------------------------------------------------------------------ |
| Feature space          | col(X): the subspace of R^n spanned by the columns of the feature matrix                        |
| Column space           | Synonym for feature space; the set of all linear combinations of the columns of X                |
| Row perspective        | Viewing each observation as a point in R^d; natural for scatter plots                            |
| Column perspective     | Viewing each feature as a vector in R^n; natural for the algebra of projection                   |
| Normal equations       | X'X * beta-hat = X'y; the algebraic expression of the orthogonality condition                    |
| Projection matrix      | P = X(X'X)^{-1}X'; maps the response vector to its least-squares prediction                     |
| Hat matrix             | Synonym for projection matrix (because P puts the "hat" on y)                                    |
| Idempotent             | A matrix satisfying P^2 = P; projecting twice equals projecting once                             |
| Centering              | Subtracting the mean from each variable; shifts the origin to the data centroid                   |
| Centering matrix       | C = I - (1/n)*1*1'; projects onto the subspace orthogonal to the constant vector                 |
| Inner product          | v'w = sum of component-wise products; measures alignment between vectors                         |
| Orthogonal             | Two vectors whose inner product is zero; perpendicular in n-dimensional space                    |
| Euclidean norm         | ||v|| = sqrt(v'v); the length of a vector                                                        |
| Pythagorean decomp.    | ||dot_y||^2 = ||dot_y-hat||^2 + ||dot_epsilon||^2 for centered data                             |
| R-squared              | ESS/TSS = cos^2(theta); proportion of variance explained by the projection                       |
| Multicollinearity      | Near-linear dependence among feature vectors; makes X'X nearly singular                          |
| Ridge regression       | Adds lambda*I to X'X to stabilize the projection; shrinks coefficients toward zero               |

---

## Companion Guidance for This Chapter

When helping a student with Chapter 7 material:

- **The geometric interpretation is the point.** The student already
  knows how to call `lm()`. This chapter reveals *what `lm()` is
  doing* geometrically. If a student treats the chapter as a formula
  sheet, redirect: "Can you explain what the projection matrix does
  in words, without using any symbols?"

- **Row vs. column is the threshold concept.** Many students find
  the column perspective initially disorienting. The row view is
  intuitive (scatter plots); the column view is where the algebra
  lives. Exercises 1 in both the book and workbook target this
  explicitly. Take time with it.

- **The Galton data carries from Chapter 2.** The student has seen
  these data before in the context of correlation and the regression
  line. Now the same data reveals the projection structure. Help the
  student connect: "The regression slope you computed in Chapter 2
  is a special case of the projection formula."

- **Centering simplifies, but understanding comes first.** Do not
  introduce centering until the student understands the general
  projection. Centering is a convenience that eliminates the
  intercept, not a prerequisite.

- **The Pythagorean decomposition is geometrically obvious.** Once
  the student sees that y-hat and epsilon are orthogonal, TSS = ESS
  + RSS is just the Pythagorean theorem. R-squared becomes the
  cosine-squared of an angle. This is a moment where geometry
  provides genuine insight.

- **Celebrate mathematical exploration.** A student who works through
  the proof that P is idempotent, verifies the normal equations by
  hand, or derives the centering matrix from first principles is
  doing productive mathematical reasoning in the Tukey-Brillinger
  tradition. Let them pursue it.

- **The OECD data illustrates overfitting.** With many correlated
  predictors and few observations, coefficient estimates are
  unstable. This motivates regularization (ridge, LASSO) without
  developing it. If the student notices the large standard errors,
  that is the right observation.
