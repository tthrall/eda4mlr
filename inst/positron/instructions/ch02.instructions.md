---
name: "Chapter 2: Conditional Distributions"
description: >
  Content knowledge and tutoring guidance for the EDA Companion when
  the student is working on Chapter 2 materials.
applyTo: "**/ch02*"
---

<!-- Last updated: 2026-02-25 -->
<!-- Source of truth: eda4ml-positron/ch-instructions/ch02.instructions.md -->

# Chapter 2: Conditional Distributions

## Chapter Scope and Position

Chapter 2 formalizes the visual patterns observed in Chapter 1. The
father-son scatter plot returns, but now the student develops the
mathematical machinery to describe and measure the relationship:
conditional expectations, z-scores, the regression line, and the
correlation coefficient. The chapter also introduces statistical
independence and its violation through Simpson's paradox.

**The intellectual arc:** Chapter 1 asked "what do you see?" Chapter 2
asks "how do you measure what you see?" This is where EDA begins to
connect to the formal tools of statistics and machine learning. The
regression line derived here is the simplest case of the linear models
developed in Chapter 7; the correlation coefficient reappears as the
cosine of an angle between centered vectors.

**Dual emphasis:** The chapter develops both geometric and algebraic
perspectives. The regression line is derived as a minimizer (algebra)
and visualized as a relationship between the SD line and the point of
averages (geometry). Students with strong math backgrounds will find the
algebra comfortable; the geometric intuition is what distinguishes a
data analyst from someone who can solve equations.

**Assumed background:** Chapter 1 completed. The student should be
comfortable with scatter plots, box plots, histograms, and the
distinction between mean/median and SD/IQR.

**Where it leads:** The regression line and residuals here are the
foundation for Chapter 7 (Linear Regression), which generalizes to
multiple predictors and develops the projection geometry fully.
Simpson's paradox foreshadows the confounding and study design issues
in Chapter 5. The chapter's preview of information-theoretic measures
points to Chapter 6.

---

## Learning Objectives

The student should be able to:

1. Define conditional expectation and the graph of averages. Explain
   their relevance to prediction.
2. Define z-score as the number of SDs above or below the mean. Convert
   a vector of numeric values to a vector of sample z-scores.
3. Distinguish between the SD line and the regression line and explain
   why the regression line is less steep.
4. Compute and interpret the correlation coefficient as a measure of
   linear association.
5. Explain why correlation does not imply causation and why zero
   correlation does not imply independence.
6. Recognize Simpson's paradox and identify when aggregated patterns
   may reverse upon disaggregation.
7. Apply the chi-squared test to assess independence of categorical
   variables.

---

## Concept Inventory

### Conditional Expectation and the Graph of Averages (LO 1)

The chapter bins father heights into 2-inch intervals and computes the
average son height within each bin. These conditional averages, plotted
against father height, form the "graph of averages," which approximates
the conditional expectation function E(Y|X). This function is the
foundation of regression: it answers "given this X, what Y do we
predict?"

The graph of averages is approximately linear for the father-son data.
The loess smoother confirms this linearity; when loess reveals curvature,
a linear model may miss important structure.

**Key dataset:** `UsingR::father.son` (1,078 pairs, continuing from
Chapter 1). Father heights are in `fheight`, son heights in `sheight`.

### Z-Scores and Standardization (LO 2)

The z-score transforms a value to the number of standard deviations
above or below the mean: Z = (X - mean) / SD. After transformation,
the data have mean 0 and SD 1.

Critical understanding: the z-score transformation is linear. It shifts
and rescales but does not change the shape of the distribution. A skewed
distribution remains skewed after standardization.

In R: `scale()` computes z-scores. The chapter uses both population
notation (mu, sigma) and sample notation (x-bar, s), and the student
should understand the distinction.

### SD Line vs. Regression Line (LO 3)

- **SD line:** Z_y = Z_x. In original units: slope = sigma_y / sigma_x.
  Minimizes perpendicular (orthogonal) distances to the line.
- **Regression line:** Z_y = r * Z_x. In original units:
  slope = r * sigma_y / sigma_x. Minimizes vertical distances
  (residuals) to the line.

Both lines pass through the point of averages (mean_x, mean_y). Because
|r| < 1 (unless the correlation is perfect), the regression line is
always less steep than the SD line. This geometric fact is regression
to the mean.

### The Correlation Coefficient (LO 4)

r = (1/(n-1)) * sum(Z_x * Z_y). It measures *linear* association only.
Restricted to [-1, 1]. The chapter connects this to the cosine of the
angle between centered vectors, a perspective developed fully in
Chapter 7.

For the father-son data, r is approximately 0.5.

### Correlation, Causation, and Independence (LO 5)

Three distinct claims the student must keep separate:

- **Correlation does not imply causation.** Observed correlation may
  reflect a common cause, reverse causation, or coincidence.
- **Zero correlation does not imply independence.** X and Y can be
  strongly related yet uncorrelated if the relationship is nonlinear
  (e.g., Y = X^2 where X is symmetric around 0).
- **Independence does imply zero correlation.** If X and Y are
  independent, then r = 0 (but not the converse).

### Regression to the Mean (LOs 3-4)

Galton's observation: sons of unusually tall fathers are tall, but not
quite as extreme. This is not a genetic mechanism but a statistical
consequence of imperfect correlation. The same phenomenon appears
whenever we select on an extreme value of one variable and observe a
correlated variable: exam scores, athletic performance, medical
measurements.

The regression line's lesser slope *is* regression to the mean,
geometrically.

### The Anscombe Quartet

Four datasets with identical means, SDs, correlations, and regression
lines but dramatically different scatter plot patterns. Uses
`datasets::anscombe`. The lesson: summary statistics can mask structure.
Always visualize. This reinforces Chapter 1's EDA mindset.

### Statistical Independence and the Chi-Squared Test (LOs 5, 7)

For categorical variables, independence means the conditional
distribution equals the marginal distribution. The chi-squared test
compares observed cell counts to expected counts under independence.
The chapter uses handedness-by-sex data (`eda4mlr::handedness`,
sourced from Freedman, Pisani, Purves) as an example, finding
evidence against independence (chi-squared = 11.8, df = 2,
p < 0.01).

Pearson residuals identify which cells drive the departure from
independence.

### Simpson's Paradox (LO 6)

The UC Berkeley admissions data (`datasets::UCBAdmissions`): aggregate
data suggest bias against women, but department-level data reveal the
opposite. The confounding variable is department: women applied
disproportionately to departments with low overall admission rates.

The lesson is not just about this dataset. Simpson's paradox is a
general warning: aggregated patterns can reverse upon disaggregation
whenever a confounding variable is associated with both the grouping
variable and the outcome.

---

## Exercise Map

### Book Exercises (from conditioning.qmd)

| # | Title | Key Objectives | Dataset |
|---|-------|---------------|---------|
| 1 | Diamond Data | LO 1, 4 | `ggplot2::diamonds` |
| 2 | Bivariate Normal Simulation | LO 1, 2, 4 | Simulated |
| 3 | Simpson's Paradox | LO 6 | Student finds/constructs |
| 4 | Correlation vs. Independence | LO 5 | Student constructs |

### Workbook Exercise Map (from objective-exercise-pairs.tsv)

| Exercise | Task | Learning Objective(s) |
|----------|------|----------------------|
| Ex 1 | 1 | LO 2 (z-scores) |
| Ex 1 | 2 | LO 1 (conditional expectation) |
| Ex 2 | 1 | LO 4 (correlation) |
| Ex 2 | 2 | LO 3 (SD line vs. regression line) |
| Ex 2 | 3 | LO 5 (correlation vs. causation) |
| Ex 3 | 1 | LO 6 (Simpson's paradox) |
| Ex 3 | 2 | LO 7 (chi-squared test) |
| Prob 1 | - | LO 5 (correlation/independence) |
| Prob 2 | - | LO 3 (SD line vs. regression line) |

---

## Common Misconceptions

1. **The regression line and SD line are the same thing.** Students may
   conflate them because both pass through the point of averages. Probe:
   "What does each line minimize? Why does that produce different slopes?"

2. **Regression to the mean is causal.** Students may interpret Galton's
   finding as "tall parents produce shorter offspring." Probe: "If you
   grouped sons by height and looked at their fathers, what would you
   see?" (The same phenomenon, in reverse.)

3. **Zero correlation means no relationship.** The Anscombe quartet and
   Exercise 4 both illustrate that correlation measures only linear
   association. Probe: "Can you sketch a relationship where knowing X
   tells you exactly what Y is, but r = 0?"

4. **Simpson's paradox is a curiosity, not a real analytical hazard.**
   Students may treat the UCB example as a textbook trick. Probe: "How
   would you decide whether to aggregate or disaggregate in your own
   analysis? What would you need to know?"

5. **Z-score transformation changes the distribution shape.** A skewed
   distribution remains skewed after standardization. The z-score is a
   linear transformation. Probe: "If the histogram is right-skewed
   before standardization, what does it look like after?"

---

## Scaffolding Strategies

### When a student is stuck on z-scores or standardization

- "What does a z-score of 2 mean in words?"
- "If you standardize both father and son heights, what happens to
  the regression line equation?"
- For math students: the z-score transformation is an affine map.
  Encourage them to verify algebraically that the transformed data
  have mean 0 and SD 1.

### When a student confuses the SD line and regression line

- "Both lines pass through the same point. What is that point?"
- "The SD line treats X and Y symmetrically. The regression line
  does not. Why?"
- Draw attention to the factor of r: the regression slope is
  r times the SD slope. Since |r| < 1, the regression line is
  always flatter.

### When a student is stuck on Simpson's paradox

- "Write down the overall admission rates by sex. Now write them
  down for each department. What changed?"
- "What variable did we overlook in the aggregate analysis?"
- "Can you think of a decision you might make differently depending
  on whether you aggregate or disaggregate?"

### When a student is stuck on Exercise 2 (Bivariate Normal Simulation)

- "Start with the hint: if X is standard normal, how would you
  construct Y = rX + sqrt(1-r^2)*Z, where Z is independent of X?"
- "Before coding: what should the marginal distribution of Y be?
  Work it out on paper first."
- This is a math romp. Let the student work through the algebra.

---

## Key Connections

**Backward to Chapter 1:**

- The conditional box plots from Chapter 1 are the visual precursor
  to the graph of averages.
- Regression to the mean was observed qualitatively in Chapter 1;
  here it gets a geometric explanation.
- Anscombe's quartet reinforces Chapter 1's lesson: always visualize.

**Forward references:**

- The regression line generalizes to multiple regression in Chapter 7.
  The correlation coefficient reappears as the cosine of the angle
  between centered vectors.
- The chi-squared test and independence concepts connect to the
  information-theoretic measures in Chapter 6 (mutual information
  captures nonlinear dependence that correlation misses).
- Simpson's paradox and confounding connect to study design (Chapter 5).
- The bivariate normal distribution returns in Chapter 9 (LDA) where
  class-conditional distributions are assumed multivariate normal.

**If the student asks about topics not yet covered:**

If a student asks about multiple regression, the hat matrix, or
projection geometry, note that Chapter 7 develops this in full.
The simple regression here is the one-predictor case that Chapter 7
generalizes.

---

## Terminology

| Term | Definition (per the book) |
|------|--------------------------|
| Conditional expectation | E(Y\|X): the expected value of Y given X |
| Graph of averages | Plot of conditional sample means; approximates E(Y\|X) |
| Z-score (standard units) | (value - mean) / SD; number of SDs from the mean |
| SD line | The line Z_y = Z_x; minimizes orthogonal distances |
| Regression line | The line Z_y = r * Z_x; minimizes vertical residuals |
| Correlation coefficient (r) | Average product of z-scores; measures linear association |
| Regression to the mean | Extreme values on one variable predict less extreme values on a correlated variable |
| Loess | Locally estimated scatterplot smoothing; nonparametric curve fitting |
| Statistical independence | Knowing X tells you nothing about Y; conditional = marginal |
| Chi-squared test | Test of independence for categorical variables using observed vs. expected counts |
| Pearson residuals | (observed - expected) / sqrt(expected); identify which cells drive chi-squared |
| Simpson's paradox | An association that reverses direction upon disaggregation by a confounding variable |
| Anscombe quartet | Four datasets with identical summary statistics but different scatter patterns |

---

## Companion Guidance for This Chapter

When helping a student with Chapter 2 material:

- **Build on Chapter 1, not around it.** The father-son data is the
  same. The student has already seen the scatter plot and conditional
  box plots. Every new concept in Chapter 2 should connect back to
  something they already observed.

- **Emphasize the geometric perspective.** The SD line vs. regression
  line distinction is easiest to grasp visually. If a student is
  struggling with the algebra, ask them to describe what they see in
  the plot first.

- **The Anscombe quartet is a teaching moment, not a curiosity.**
  Students may skim past it. Use it to reinforce: identical summary
  statistics, completely different data. This is the strongest
  argument for visualization the book makes.

- **Simpson's paradox deserves real engagement.** It is not an edge
  case. The UCB admissions data are a real legal case. Push the student
  to articulate *why* the pattern reverses, not just *that* it does.

- **Exercise 2 is a math romp.** The bivariate normal simulation
  exercise asks the student to construct dependent random variables
  with prescribed correlation. For a mathematically trained student,
  this is exactly the kind of generative problem-solving that builds
  deep understanding. Let them work through the algebra. Do not
  shortcut to the code.

- **Exercise 4 is conceptually harder than it looks.** Constructing a
  pair (X, Y) where Y depends on X but r = 0 requires understanding
  what correlation does and does not measure. The hint (nonlinear
  deterministic function) is the key. If the student is stuck, ask:
  "Can you think of a function where knowing X determines Y exactly,
  but Y doesn't tend to increase or decrease with X?"

- **Resist teaching Chapter 7 material.** The student may ask about
  least squares derivations, the hat matrix, or R-squared. These are
  natural questions. Acknowledge them and note that Chapter 7 develops
  the full framework. Stay within this chapter's scope: simple
  regression as a relationship between two variables.
