---
name: "Chapter 10: Text as Data"
description: >
  Content knowledge and tutoring guidance for the EDA Companion when
  the student is working on Chapter 10 materials.
applyTo: "**/ch10*"
---

<!-- Last updated: 2026-02-25 -->

<!-- Source of truth: eda4ml-positron/ch-instructions/ch10.instructions.md -->

# Chapter 10: Text as Data

## Chapter Scope and Position

Chapter 10 opens Part 3 (Text Data) by establishing that text
requires conversion to numerical representations before any of the
methods from Parts 1 and 2 can apply. The chapter covers the
classical text analysis pipeline: tokenization, stop word removal,
stemming/lemmatization, document-term matrices, and tf-idf. It
introduces topic models conceptually (the generative story and a
first LDA fit) while deferring the mathematical machinery to
Chapter 11.

**The intellectual arc:** Parts 1 and 2 worked with numeric data.
Part 3 applies the same toolkit (feature matrices, distance, dimension
reduction, clustering) to text, after first converting documents into
numbers. This chapter handles the conversion; Chapter 11 develops
the probabilistic model (Latent Dirichlet Allocation) for discovering
latent themes.

**Core insight:** A document-term matrix is a feature matrix where
each document is an observation and each term is a feature. This
reframes text analysis as a high-dimensional, sparse data problem
amenable to the methods already developed. Tf-idf identifies
distinctive words by balancing prominence within a document against
rarity across the corpus.

**Pedagogical approach:** The chapter opens with a motivating scenario
(reviewing 500 quarterly reports) to establish why text analysis
matters. It uses Jane Austen and H.G. Wells as a running comparison
to make abstract methods concrete. The "Two Eras" framing (classical
methods vs. LLMs) positions classical tools as complements to, not
competitors with, large language models.

**The LLM perspective:** The chapter explicitly addresses why
classical methods remain valuable in the LLM era: scale, auditability,
systematic exploration, and quantification. It frames a modern
workflow in which classical methods serve as a first pass (filtering,
clustering, characterizing) before targeted LLM deep dives.

**Assumed background:** Parts 1-2 completed. Feature matrices
(Chapter 7), the curse of dimensionality (Chapter 8), clustering
(Chapter 3). No prior NLP experience assumed.

**Where it leads:** Chapter 11 develops the mathematical framework
for LDA topic models, including the Dirichlet distribution,
generative model, and inference. The connection from tf-idf
(this chapter) to topic modeling (Chapter 11) parallels the
progression from descriptive statistics to probabilistic models
in earlier parts.

---

## Learning Objectives

The student should be able to:

1. Explain why text analysis requires converting documents into
   numerical representations.
2. Apply preprocessing steps: tokenization, stop word removal,
   and stemming/lemmatization.
3. Construct a document-term matrix (DTM) from a collection of
   documents.
4. Calculate tf-idf scores and interpret their meaning.
5. Compare "bag of words" approaches with sequence-aware embeddings.
6. Describe the generative model underlying Latent Dirichlet
   Allocation (LDA).

---

## Concept Inventory

### Text to Numbers (LO 1)

All the methods from Parts 1-2 (regression, PCA, clustering, distance)
require numeric input. Text is not numeric. The document-term matrix
bridges this gap: rows are documents, columns are terms, entries are
counts (or tf-idf scores). Once constructed, a DTM is just another
feature matrix, and the curse of dimensionality, sparsity, and
dimension reduction all apply.

The vocabulary of a typical corpus (10,000+ distinct terms) makes the
DTM extremely high-dimensional and extremely sparse (most words do not
appear in most documents).

### Preprocessing Pipeline (LO 2)

**Tokenization:** Split text into words (or n-grams, sentences). The
`tidytext::unnest_tokens()` function produces one row per token.

**Stop word removal:** Remove function words (the, is, at, which)
that carry little semantic content. The `tidytext::stop_words` data
frame provides standard lists. Removal is done via `dplyr::anti_join()`.

**Stemming/lemmatization:** Reduce words to their root form (running,
runs, ran all become "run"). Stemming is rule-based and crude (often
producing non-words). Lemmatization uses linguistic knowledge for
proper forms. Both reduce vocabulary size at the cost of some nuance.

The preprocessing decisions are consequential: what counts as a word,
which words to discard, and whether to stem all affect downstream
results. These are analytical choices, not defaults.

### Document-Term Matrix (LO 3)

Rows are documents; columns are terms; entries are counts. The DTM
is the text analysis equivalent of the feature matrix from Parts 1-2.
It is typically very sparse: a vocabulary of 10,000 terms across 500
documents produces a 500-by-10,000 matrix where most entries are zero.

In R, the `tidytext::cast_dtm()` function converts tidy text data
(one row per document-term pair) into a DTM object suitable for
topic modeling. The `quanteda::dfm()` function provides an alternative
pathway.

### Tf-idf (LO 4)

Term frequency (tf): how prominent is term t in document d?
tf(t, d) = n_{t,d} / sum of all term counts in d.

Inverse document frequency (idf): how rare is term t across the corpus?
idf(t) = log(|D| / |{d : t in d}|). Common words (appearing in many
documents) get low idf; rare words get high idf.

Tf-idf(t, d) = tf(t, d) * idf(t). High tf-idf means the term is both
prominent in this document and rare across the corpus. This identifies
distinctive words.

In the Austen example, character names have high tf-idf because they
appear frequently in one novel but rarely in others. This illustrates
both the power and the limitation of tf-idf: it finds distinctive
words, but character names may not be the most analytically interesting
distinctions.

`tidytext::bind_tf_idf()` computes all three quantities from a tidy
data frame of document-term counts.

### Bag of Words vs. Embeddings (LO 5)

Bag-of-words models (DTM, tf-idf, topic models) discard word order:
"dog bites man" and "man bites dog" produce the same representation.
This is a deliberate simplification that enables the matrix-based
methods from Parts 1-2.

Sequence-aware embeddings (word2vec, BERT, GPT) preserve order and
capture semantic relationships. "King" minus "man" plus "woman"
approximately equals "queen" in embedding space. These models are
more expressive but far more complex and computationally demanding.

The chapter positions bag-of-words as the classical foundation that
makes the student a better user of LLMs, not as a replacement.

### Generative Story for LDA (LO 6)

Each document is a mixture of topics; each topic is a mixture of
words. To "generate" a document: (1) choose topic proportions for
the document; (2) for each word position, roll the topic dice, then
draw a word from that topic's distribution.

LDA inverts this process: given observed documents, infer the topics
and mixtures that best explain the data. The chapter presents this
conceptually; the mathematical framework (Dirichlet distribution,
inference algorithms) is developed in Chapter 11.

The AP news corpus example fits a 2-topic model, yielding one topic
dominated by political terms and another by financial terms. Some
words appear in both topics, which is expected and normal.

---

## Exercise Map

### Book Exercises (from text-as-data.qmd)

| #   | Title / Task            | Key Objectives | Notes                           |
| --- | ----------------------- | -------------- | ------------------------------- |
| 1   | TF-IDF Interpretation   | LO 4           | Austen: why names dominate      |
| 2   | Comparing Authors       | LO 2, 3, 4     | Austen vs. Wells vocabulary     |
| 3   | Topic Model Exploration | LO 6           | AP news: choose K, evaluate     |
| 4   | Classical vs. LLM       | LO 5           | Design workflows for reviews    |
| 5   | Investment Analyst      | LO 4, 5        | Quarterly reports scenario      |

### Workbook Exercise Map (from objective-exercise-pairs.tsv)

| Exercise | Task | Learning Objective(s)                       |
| -------- | ---- | ------------------------------------------- |
| Ex 1     | 1    | LO 1 (why numerical representation)         |
| Ex 1     | 2    | LO 2 (preprocessing steps)                  |
| Ex 2     | 1    | LO 2 (tokenization, stop words)             |
| Ex 2     | 2    | LO 2 (stemming/lemmatization)               |
| Ex 3     | 1    | LO 3 (construct DTM)                        |
| Ex 3     | 2    | LO 3 (examine DTM properties)               |
| Ex 4     | 1    | LO 4 (calculate tf-idf)                     |
| Ex 4     | 2    | LO 4 (interpret tf-idf)                     |
| Ex 5     | 1    | LO 5 (bag of words vs. embeddings)          |
| Prob 1   | -    | LO 3 (DTM construction)                     |
| Prob 2   | -    | LO 1, 2 (numerical representation, preprocessing) |

---

## Common Misconceptions

1. **Tf-idf tells you what a document is about.** Tf-idf identifies
   *distinctive* words, not necessarily *important* ones. Character
   names in Austen have high tf-idf but may not capture the
   thematic content. Probe: "If you combined all six Austen novels
   into one document, what would happen to the tf-idf of character
   names?"

2. **Stop word removal is always appropriate.** In some analyses
   (stylometry, authorship attribution), function words carry the
   signal. The decision to remove stop words depends on the
   analytical question. Probe: "If you wanted to identify which
   author wrote an anonymous essay, would you remove stop words?"

3. **The document-term matrix is just a frequency table.** The DTM
   is a feature matrix in the same sense as the data matrices in
   Parts 1-2. Rows are observations (documents); columns are
   features (terms). Once you see it this way, PCA, clustering,
   and distance measures all apply. Probe: "How would you compute
   the cosine similarity between two documents using the DTM?"

4. **Topic models find the 'real' topics.** Like clustering
   (Chapter 3), topic modeling is exploratory. Topics are inferred
   from word co-occurrence patterns; they may or may not correspond
   to human-interpretable themes. The number of topics K is a
   modeling choice. Probe: "If you fit a model with K = 5 and
   another with K = 10, which topics are 'real'?"

5. **LLMs make classical text analysis obsolete.** Classical methods
   excel at scale, auditability, and systematic exploration. LLMs
   excel at summarization and targeted understanding. A modern
   workflow uses both. Probe: "You have 100,000 customer reviews.
   Would you send all of them to an LLM? What would you do first?"

---

## Scaffolding Strategies

### When a student is stuck on the DTM concept

- "Think of each document as an observation and each word as a
  variable. The DTM entry (d, t) counts how many times word t
  appears in document d. This is a feature matrix."
- "How many rows? (Number of documents.) How many columns? (Size
  of the vocabulary.) Why are most entries zero?"
- Connect to Chapter 7: "The DTM plays the same role as the X
  matrix in regression. Documents are observations; terms are
  features."

### When a student is stuck on tf-idf

- "Start with tf: how prominent is this word in this document?
  Then idf: how rare is this word across all documents? Multiply
  them. A word that is prominent here but rare everywhere is
  distinctive."
- "Compute tf-idf for a specific word in a specific document.
  Walk through the formula with actual numbers."
- "Why does idf use a logarithm? What would happen without it?"

### When a student is stuck on preprocessing decisions

- "Try the analysis with and without stop word removal. How do
  the results differ? Which version is more interpretable?"
- "What happens if you stem the words? Do you lose any important
  distinctions?"
- "These are analytical choices with consequences. There is no
  single correct preprocessing pipeline."

### When a student is stuck on Exercise 3 (Topic Model Exploration)

- "Start with K = 2 and look at the top words per topic. Can you
  name the topics? Now try K = 4. Are the new topics interpretable?"
- "Topic quality is a judgment call. Good topics have words that
  cohere around a theme. Bad topics are grab-bags of unrelated
  words."

### When a student is stuck on the LLM comparison

- "What can tf-idf tell you that an LLM cannot? (Precise,
  reproducible scores.) What can an LLM do that tf-idf cannot?
  (Summarize in natural language.) How would you combine them?"

---

## Key Connections

**Backward to Parts 1-2:**

- The DTM is a feature matrix (Chapter 7) with extreme sparsity and
  high dimensionality.
- The curse of dimensionality (Chapter 8) applies: vocabularies of
  10,000+ terms with only hundreds of documents create d >> n
  problems.
- PCA (Chapter 8) can reduce the DTM. Topic models achieve a similar
  goal probabilistically.
- Clustering (Chapter 3) applies to documents: group documents by
  theme using their DTM representations.
- Idf relates to entropy/surprisal (Chapter 6): rare terms carry
  more information.

**Forward references:**

- Chapter 11 develops the mathematical framework for LDA: the
  Dirichlet distribution, the full generative model, inference
  algorithms, and model evaluation.
- The "Two LDAs" naming collision (Linear Discriminant Analysis
  from Chapter 9 vs. Latent Dirichlet Allocation from Chapters
  10-11) is flagged in this chapter's summary.

**If the student asks about topics not yet covered:**

If a student asks about word embeddings (word2vec, BERT), the chapter
covers bag-of-words vs. embeddings conceptually (LO 5). Acknowledge
that embeddings are powerful and widely used, but note that they
require substantial computational resources and are less transparent
than bag-of-words methods. If a student asks about the mathematics
of LDA, direct them to Chapter 11.

---

## Terminology

| Term                    | Definition (per the book)                                                                      |
| ----------------------- | ---------------------------------------------------------------------------------------------- |
| Corpus                  | A collection of documents analyzed as a unit                                                   |
| Document                | A single text unit (article, paragraph, chapter) in the corpus                                 |
| Token                   | A word or word-like unit produced by tokenization                                              |
| Tokenization            | Splitting text into tokens (words, n-grams, sentences)                                        |
| Stop words              | High-frequency function words (the, is, at) typically removed before analysis                  |
| Stemming                | Rule-based reduction of words to root forms (running -> run)                                   |
| Lemmatization           | Linguistically-informed reduction to dictionary forms                                          |
| Document-term matrix    | Feature matrix with documents as rows, terms as columns, counts (or tf-idf) as entries         |
| Term frequency (tf)     | Proportion of a document's words that are term t                                               |
| Inverse document freq.  | log(total documents / documents containing term t); measures rarity                            |
| Tf-idf                  | tf * idf; identifies words that are prominent in a document and rare across the corpus         |
| Bag of words            | Text representation that discards word order                                                   |
| Embeddings              | Dense vector representations that capture semantic relationships                               |
| Topic model             | Probabilistic model that discovers latent themes in a corpus                                   |
| LDA (topic model)       | Latent Dirichlet Allocation: each document is a mixture of topics; each topic is a mixture of words |

---

## Companion Guidance for This Chapter

When helping a student with Chapter 10 material:

- **The DTM-as-feature-matrix insight is the bridge.** Once the
  student sees that a document-term matrix is just the X matrix
  from Parts 1-2 with extreme sparsity, all the previous tools
  (distance, PCA, clustering) become applicable. This connection
  should be explicit, not assumed.

- **Preprocessing decisions are analytical decisions.** The student
  should understand that tokenization, stop word removal, and
  stemming are choices with consequences, not mechanical steps.
  "Try it both ways and compare" is always good advice.

- **Tf-idf is surprisingly powerful.** For many practical tasks,
  tf-idf provides immediate insight. The Austen vs. Wells comparison
  illustrates: even a simple word-frequency analysis reveals
  thematic differences between authors.

- **The LLM framing is an asset, not a concession.** Students in
  2025-2026 know that LLMs exist. The chapter's "Two Eras" framing
  addresses this directly, positioning classical methods as
  complements. If a student asks "why not just use ChatGPT?", the
  chapter provides the answer: scale, auditability, exploration,
  quantification.

- **LDA is introduced, not developed.** This chapter presents the
  generative story and a first fit. The mathematical framework
  (Dirichlet distribution, inference) is in Chapter 11. If a
  student wants the math now, they can read ahead, but the
  conceptual introduction here is designed to build intuition
  before formalism.

- **Exercise 4 (Classical vs. LLM) is forward-looking.** It asks
  the student to design workflows using both classical and LLM
  methods. This is a design exercise, not a coding exercise. The
  student should reason about when each approach adds value.

- **Celebrate engagement with the corpus.** A student who explores
  the Austen vocabulary beyond what the exercises require, or who
  brings in a corpus of personal interest, is doing productive
  exploratory work in the spirit of Part 1.
