---
name: "Chapter 5: Sampling and Study Design"
description: >
  Content knowledge and tutoring guidance for the EDA Companion when
  the student is working on Chapter 5 materials.
applyTo: "**/ch05*"
---

<!-- Last updated: 2026-02-25 -->

<!-- Source of truth: eda4ml-positron/ch-instructions/ch05.instructions.md -->

# Chapter 5: Sampling and Study Design

## Chapter Scope and Position

Chapter 5 steps back from analysis techniques to ask a prior question:
where did the data come from, and what can it support? The chapter
introduces sampling theory, study design, and the distinction between
observational and experimental studies through historical examples.
It concludes with a treatment of analyst degrees of freedom, showing
that even with good data, analytical choices can lead to divergent
conclusions.

**The intellectual arc:** Chapters 1-4 developed tools for exploring
and summarizing data. Chapter 5 asks: "Can you trust the data you are
exploring?" This is where EDA connects to the broader practice of
data science. The capabilities of an ML system are bound by the quality
of the data used to train, test, and evaluate it.

**Pedagogical approach:** The chapter follows Freedman, Pisani, and
Purves (FPP) and teaches through historical case studies rather than
abstract theory. Each example illustrates a principle: the Literary
Digest poll (selection bias), the Salk vaccine trial (randomization),
NB10 (measurement precision), and the Silberzahn study (analyst
degrees of freedom). The cases are memorable and make abstract
concepts concrete.

**ML relevance:** Every historical example maps to a modern ML concern.
Selection bias → training data that does not represent the deployment
environment. Confounding → features that correlate with both the
treatment and outcome. Measurement error → noisy labels. Analyst
degrees of freedom → the garden of forking paths in model selection.

**Assumed background:** Chapters 1-4 completed. The mean-vs-median
robustness from Chapters 1 and 4. The standard error formula uses
concepts from Chapter 4 (sampling distributions).

**Where it leads:** The confounding issues raised here connect directly
to Simpson's paradox (Chapter 2) and motivate the careful data
practices needed for supervised learning (Chapters 7-9). The analyst
degrees of freedom section foreshadows the train/validate/test
discipline that recurs throughout the modeling chapters.

---

## Learning Objectives

The student should be able to:

1. Explain why sample size alone does not guarantee valid inference.
2. Give an example of selection bias in an observational study and
   its consequences.
3. Distinguish between observational and experimental study designs.
4. Define a sampling frame. Explain how it affects generalizability
   to a target population.
5. Define and give examples of bias, chance error, and measurement
   uncertainty.
6. Recognize how analyst degrees of freedom can lead to divergent
   conclusions from identical data.
7. Calculate the standard error of a sample mean and interpret its
   implications for estimation precision.

---

## Concept Inventory

### The Literary Digest Poll, 1936 (LOs 1, 2, 4)

The Literary Digest polled 2.4 million respondents and predicted
Landon would defeat Roosevelt. Gallup polled 50,000 and correctly
predicted Roosevelt. The Digest's error (nearly 20 percentage points)
remains the largest in history by a major poll.

**Key dataset:** `eda4mlr::lit_digest` — a small table showing predicted
percentages for FDR from three sources.

The failure was not sample size (2.4 million is enormous) but the
sampling frame: the Digest drew names from telephone directories and
automobile registrations, systematically overrepresenting wealthier
(and Republican-leaning) households. The resulting selection bias could
not be corrected by making the sample larger.

**ML connection:** Training data that does not represent the deployment
environment. A model trained on Digest respondents would learn patterns
that fail to generalize.

### Truman vs. Dewey, 1948 (LOs 1, 2)

Pollsters predicted Dewey would defeat Truman. The error came from
quota sampling (interviewers chose whom to interview within demographic
quotas) and from stopping polling too early. Gallup later adopted
probability sampling.

**Key dataset:** `eda4mlr::truman_dewey` — predicted vs. actual results.

### The Salk Vaccine Trial (LO 3)

Two designs were used: the NFIP design (second-graders vaccinated,
first/third-graders as controls) and the randomized double-blind design
(children randomly assigned to vaccine or placebo, neither parents nor
evaluators knew which). The randomized design was more convincing because
it controlled for confounding factors (e.g., socioeconomic status
correlated with both consent and polio risk).

**Key datasets:** `eda4mlr::salk_blind` (randomized design results) and
`eda4mlr::salk_nfip` (NFIP design results).

**ML connection:** A/B testing and randomized experiments are the gold
standard for causal inference in both medicine and technology.

### Portacaval Shunt (LO 3)

A surgical treatment evaluated across 51 studies with varying designs:
no controls, nonrandomized controls, and randomized controls. Studies
without randomization consistently showed more favorable results.

**Key datasets:** `eda4mlr::portacaval_studies` (design types and
outcomes) and `eda4mlr::portacaval_survival` (survival rates by design).

The lesson: study design determines what conclusions are warranted.
More rigorous designs (randomized controls) produced less optimistic
but more reliable results.

### Repeated Weighing of NB10 (LOs 5, 7)

NB10 is a standard weight nominally weighing 10 grams. A series of
100 measurements reveals that it weighs about 405 micrograms less
than 10 grams, with a standard deviation of about 6.5 micrograms
across measurements.

**Key dataset:** `eda4mlr::nb10` — 100 measured deficits from 10 grams.

The NB10 example motivates the standard error of the sample mean.
Each measurement = true deficit + chance error. The standard error
of the average is SD/√n = 6.5/√100 = 0.65 micrograms. The true
deficit is known to within about 2 micrograms.

### Standard Error of the Sample Mean (LO 7)

For independent observations D_1, ..., D_n with common SD σ:

- SD of the sum = σ√n
- SD of the average = σ/√n (the standard error)

The standard error quantifies how precise the sample average is as
an estimate of the population mean. It decreases with √n, not n:
quadrupling the sample size halves the standard error.

### Analyst Degrees of Freedom (LO 6)

The Silberzahn et al. (2018) study: 29 independent research teams
analyzed the same dataset (soccer red cards and player skin tone).
Teams used 21 distinct covariate combinations. Effect size estimates
ranged from 0.89 to 2.93 in odds-ratio terms. Some found significant
effects; others did not. All teams were competent; all had identical
data.

The explanation: defensible analytical choices (data cleaning, variable
selection, model specification, statistical framework) compound
multiplicatively. Two options at each of five decision points yield
2^5 = 32 possible analytical paths.

Holmes (2018) extends this with the ELISA example: a positive result
at 4/50 patients at one protein site has p = 0.00175, but if the
researcher tested 60 candidate sites and reported the best, the
probability of a maximum this extreme is approximately 0.10.

**ML connection:** Feature engineering, model selection, hyperparameter
tuning, and evaluation choices are all analyst degrees of freedom.
The train/validate/test split institutionalizes the separation between
exploratory and confirmatory analysis.

### The Role of EDA

The chapter concludes by positioning EDA between study design and
model development. EDA reveals whether design intentions were achieved:
whether the feature distribution matches expectations, whether
missing data patterns suggest bias, whether labels are reliable,
whether outliers warrant investigation.

---

## Exercise Map

### Book Exercises (from study-design.qmd)

| #   | Title                             | Key Objectives | Notes                                         |
| --- | --------------------------------- | -------------- | --------------------------------------------- |
| 1   | Observational Studies in Practice | LO 1, 2, 3     | Team discussion; identify real-world examples |
| 2   | Quiz Score Arithmetic             | LO 5, 7        | Deductive; no data needed                     |
| 3   | HANES Left-handedness             | LO 2, 4        | Survivorship/cohort effect                    |
| 4   | Height Percentiles                | LO 7           | Normal distribution calculation               |

### Workbook Exercise Map (from objective-exercise-pairs.tsv)

| Exercise | Task | Learning Objective(s)                 |
| -------- | ---- | ------------------------------------- |
| Ex 1     | 1    | LO 1 (sample size ≠ validity)         |
| Ex 1     | 2    | LO 2 (selection bias)                 |
| Ex 2     | 1    | LO 3 (observational vs. experimental) |
| Ex 2     | 2    | LO 3 (observational vs. experimental) |
| Ex 2     | 3    | LO 3 (observational vs. experimental) |
| Ex 3     | 1    | LO 4 (sampling frame)                 |
| Ex 3     | 2    | LO 4 (sampling frame)                 |
| Ex 4     | 1    | LO 5 (bias, chance error)             |
| Ex 4     | 2    | LO 5 (measurement uncertainty)        |
| Prob 1   | -    | LO 3 (study design)                   |
| Prob 2   | -    | LO 4, 5 (sampling frame, measurement) |

---

## Common Misconceptions

1. **Large sample size guarantees accurate results.** The Literary
   Digest had 2.4 million respondents and was catastrophically wrong.
   Sample size cannot overcome systematic bias in the sampling frame.
   Probe: "The Digest had 48 times more respondents than Gallup. Why
   was Gallup more accurate?"

2. **Observational studies can establish causation.** Observational
   studies show association; only randomized experiments support causal
   claims (with caveats). The portacaval shunt studies illustrate this
   directly. Probe: "Why did studies without randomization find more
   favorable results? What confounders might explain the difference?"

3. **Randomization is unnecessary when you control for confounders.**
   You can only control for confounders you know about and can measure.
   Randomization balances both known and unknown confounders. Probe:
   "In the Salk trial, what confounders might the NFIP design have
   missed that randomization would address?"

4. **The standard error and the standard deviation are the same thing.**
   SD measures the spread of individual observations. SE measures the
   precision of the sample mean. SE = SD/√n. They differ by a factor
   of √n. Probe: "If you double the sample size, what happens to the
   SD? To the SE?"

5. **Analyst degrees of freedom is the same as p-hacking.** The
   Silberzahn study shows that defensible, good-faith analytical
   choices can produce divergent results. This is not fraud or
   manipulation; it is the natural consequence of methodological
   flexibility. Probe: "Were any of the 29 teams wrong? What does
   it mean that competent analysts reached different conclusions?"

---

## Scaffolding Strategies

### When a student is stuck on the Literary Digest example

- "Who was in the Digest's sample? Who was not? How might those
  two groups differ in their political preferences?"
- "If you wanted to predict an election, what population would
  your sample need to represent?"
- Connect to ML: "If your training data came from the Digest's
  sampling frame, what kind of predictions would your model make?"

### When a student is stuck on observational vs. experimental (Ex 1)

- "In your workplace example, who decides which 'treatment' each
  subject receives? Is it the researcher or the subject?"
- "What is the key advantage of randomization? What does it control
  for that matching or stratification cannot?"

### When a student is stuck on Exercise 2 (Quiz Scores)

- "If the average number right is 6.4 out of 10, what must the
  average number wrong be?"
- "The number right + number wrong = 10 for every student. What
  does this constraint tell you about the SDs?"
- This is a deductive exercise. The data are not needed; the
  arithmetic determines the answer.

### When a student is stuck on Exercise 3 (Left-handedness)

- "The data show that the percentage of left-handed people decreases
  with age. Does this mean people switch from left to right as they
  age?"
- "Think about when these people were born. Were attitudes toward
  left-handedness the same in 1920 as in 1970?"
- The answer involves survivorship/cohort effects, not individual
  change. Older cohorts were more likely to have been forced to
  switch to right-handedness.

### When a student is stuck on the standard error

- "The sample average of the NB10 deficits is 405 micrograms. Is
  this exactly the true deficit? How far off might it be?"
- "SE = SD/√n. If the SD of individual measurements is 6.5 and
  n = 100, what is the SE?"
- "If you took 400 measurements instead of 100, how would the
  SE change?"

### When a student is stuck on analyst degrees of freedom

- "List the decisions you would need to make to analyze the soccer
  data. How many options exist at each decision point?"
- "If 29 teams reach different conclusions from identical data,
  what does that tell you about the certainty of any single
  analysis?"
- Connect to ML: "Where in a machine learning pipeline do you
  face similar choices?"

---

## Key Connections

**Backward to Chapters 1-4:**

- Simpson's paradox (Chapter 2) is a specific instance of
  confounding, which this chapter treats as a general study design
  issue.
- The mean-vs-median robustness (Chapters 1, 4) connects to
  measurement sensitivity: outliers in data may reflect measurement
  problems, not population features.
- The bootstrap (Chapter 4) gives the student a tool for
  quantifying uncertainty that complements the SE formula here.
- The sampling distribution idea (Chapter 4) is the conceptual
  foundation for the standard error.

**Forward references:**

- The train/validate/test split (Chapters 7-9) institutionalizes
  the separation between exploratory and confirmatory analysis
  that this chapter argues for.
- Selection bias and confounding return in every supervised
  learning chapter where the student must consider whether training
  data represent the deployment context.
- Analyst degrees of freedom motivate sensitivity analysis and
  pre-registration, practices the book endorses throughout.

**If the student asks about topics not yet covered:**

If a student asks about A/B testing, causal inference methods
(instrumental variables, difference-in-differences), or MLOps,
acknowledge these are important extensions. The chapter provides
the conceptual foundation; specialized treatments are beyond its
scope.

---

## Terminology

| Term                        | Definition (per the book)                                                                     |
| --------------------------- | --------------------------------------------------------------------------------------------- |
| Sampling frame              | The list or mechanism from which sample members are drawn                                     |
| Selection bias              | Systematic difference between sample and population that affects the quantity of interest     |
| Observational study         | Subjects are observed as they are; no treatment is assigned by the researcher                 |
| Experimental study          | The researcher assigns treatments, ideally through randomization                              |
| Randomized controlled trial | Experiment where treatment assignment is random and includes a control group                  |
| Double-blind                | Neither subjects nor evaluators know treatment assignments                                    |
| Confounding variable        | A variable associated with both the treatment and the outcome, creating spurious associations |
| Bias                        | Systematic error that does not decrease with sample size                                      |
| Chance error                | Random variation that decreases with √n                                                       |
| Standard error              | SD of a statistic's sampling distribution; for the mean, SE = σ/√n                            |
| Analyst degrees of freedom  | The set of defensible analytical choices available at each step of an analysis                |
| Garden of forking paths     | The branching tree of analytical decisions, most of which are invisible in the final report   |

---

## Companion Guidance for This Chapter

When helping a student with Chapter 5 material:

- **The historical examples are the content, not decoration.** Each
  case study illustrates a principle that students will encounter in
  their own practice. If a student wants to skip the history, redirect:
  "The Literary Digest made a mistake that data scientists still make
  today. What was it, and how would you avoid it?"

- **Connect every example to ML practice.** The chapter makes these
  connections explicitly (ML Connection callouts). Reinforce them:
  "If your training data had the same bias as the Digest's sample,
  what would happen to your model's predictions?"

- **Exercise 2 is deductive, not computational.** The quiz score
  problem requires no data and no code. The student must reason from
  the constraint (right + wrong = 10 for every student) to determine
  the missing values. This is a good test of whether they understand
  the summary statistics from Chapter 1 at a deeper level.

- **Exercise 3 is a trap for naive interpretation.** The declining
  percentage of left-handedness with age is not about individual
  change; it is a cohort effect. Students who jump to the conclusion
  that people switch handedness as they age are misinterpreting cross-
  sectional data to make a longitudinal inference. This connects
  to the survivorship bias theme.

- **The analyst degrees of freedom section may feel abstract.**
  Ground it in practice: "Next time you make a modeling choice
  (which features to include, which algorithm to use), notice that
  you are at a fork in the garden of forking paths. What alternative
  would you have chosen? Would it change your conclusion?"

- **The NB10 example is beautifully concrete.** A physical object
  weighed 100 times. The student can see the histogram, compute the
  mean and SD, and derive the SE. If they are struggling with the
  standard error concept, start here rather than with abstract
  formulas.

- **This chapter is not a coding chapter.** The datasets are small
  and the exercises emphasize reasoning over computation. If a
  student wants to write code, that is fine, but the learning
  objectives are about understanding data quality, study design,
  and the limits of inference, not about R skills.

- **Resist downplaying analyst degrees of freedom.** Students may
  dismiss the Silberzahn study as an edge case. It is not. Every
  analysis involves choices. The chapter's prescription (transparency,
  sensitivity analysis, honest communication of uncertainty) is
  practical advice the student should internalize.
