---
name: "Chapter 13: Time Domain Methods"
description: >
  Content knowledge and tutoring guidance for the EDA Companion when
  the student is working on Chapter 13 materials.
applyTo: "**/ch13*"
---

<!-- Last updated: 2026-02-25 -->

<!-- Source of truth: eda4ml-positron/ch-instructions/ch13.instructions.md -->

# Chapter 13: Time Domain Methods

## Chapter Scope and Position

Chapter 13 is the forecasting chapter. It translates the diagnostic
tools developed in Chapter 12 (ACF, PACF, stationarity assessment)
into a family of models that exploit autocorrelation structure for
prediction. The chapter emphasizes the decision-support side of the
dual aims.

**The intellectual arc:** Chapter 12 established that time series have
memory and introduced tools to characterize that memory. Chapter 13
asks: how do we *exploit* that memory? If the past predicts the future,
what is the best way to weight past observations? The answer is the
ARIMA family: AR models weight past values, MA models weight past
shocks, and combinations achieve parsimony. The models are linear but
capture a wide range of real-world behavior.

**Pedagogical approach:** The chapter builds from simple to complex:
AR(1) to AR(p), MA(1) to MA(q), then ARMA, ARIMA, and SARIMA. Each
model type has a diagnostic signature (ACF/PACF pattern) that links
back to Chapter 12. The Box-Jenkins workflow (identify, estimate,
diagnose, iterate) provides a practical framework. Exponential
smoothing is revealed as the optimal forecast for a specific ARIMA
model, connecting a widely-used heuristic to rigorous theory.

**Assumed background:** Chapter 12 (ACF, PACF, stationarity, back-shift
operator). Linear algebra basics from Chapter 7 are helpful for
understanding the operator polynomial notation.

**Where it leads:** Chapter 14 takes the complementary frequency-domain
perspective. Students pursuing forecasting will encounter extensions
(state-space models, neural forecasting) that build on the ARIMA
foundation. The Box-Jenkins workflow exemplifies model-building
discipline applicable across statistical learning.

---

## Learning Objectives

The student should be able to:

1. Define AR(p) and MA(q) processes and identify their ACF/PACF
   signatures. (LO 1)
2. Fit an ARIMA model and interpret its parameters. (LO 2)
3. Generate forecasts from a fitted ARIMA model and explain prediction
   intervals. (LO 3)
4. Apply differencing to achieve stationarity. (LO 4)
5. Diagnose model adequacy by checking that residuals approximate
   white noise. (LO 5)
6. Compare models using information criteria (AIC, BIC). (LO 6)
7. Describe the Box-Jenkins methodology for time series
   modeling. (LO 7, book only)
8. Connect exponential smoothing to its underlying ARIMA
   representation. (LO 8, book only)

---

## Concept Inventory

| Concept | Key LOs |
|---------|---------|
| AR(1): simplest memory, geometric ACF decay, PACF cutoff at 1 | LO 1 |
| AR(p): PACF cutoff at lag p, stationarity conditions (roots outside unit circle) | LO 1 |
| MA(1): finite memory, ACF cutoff at 1, PACF decays | LO 1 |
| MA(q): ACF cutoff at lag q, invertibility conditions | LO 1 |
| ARMA(p,q): both ACF and PACF decay, parsimony | LO 1, 2 |
| Back-shift operator, AR/MA polynomial notation | LO 1, 2 |
| Differencing operator, seasonal differencing | LO 4 |
| ARIMA(p,d,q): ARMA applied to differenced series | LO 2, 4 |
| SARIMA: seasonal AR, MA, and differencing components | LO 2 |
| Random walk with drift as ARIMA(0,1,0) | LO 2, 4 |
| Model fitting: maximum likelihood estimation | LO 2 |
| Forecast as conditional expectation, recursive computation | LO 3 |
| Prediction intervals widen with horizon | LO 3 |
| MSPE and the psi-weights | LO 3 |
| Residual diagnostics: ACF of residuals, Ljung-Box test | LO 5 |
| AIC, BIC: balancing fit and complexity | LO 6 |
| Box-Jenkins workflow: identify, estimate, diagnose, iterate | LO 7 |
| Simple exponential smoothing as optimal forecast for IMA(1,1) | LO 8 |
| Holt-Winters and ETS models | LO 8 |

---

## Exercise Map

### Book Exercises (Chapter 13)

| Exercise | Topic | Key LOs |
|----------|-------|---------|
| 1 | AR(1) simulation: verify ACF/PACF theory | LO 1 |
| 2 | MA(1) simulation: contrast with AR(1) patterns | LO 1 |
| 3 | Recruitment AR modeling: compare AR(1), AR(2), AR(3) via AIC | LO 1, 6 |
| 4 | Global temperature ARIMA: differencing, forecasting | LO 2, 3, 4 |
| 5 | Airline passengers: seasonal identification, SARIMA | LO 2, 4, 5 |
| 6 | Prediction interval coverage simulation | LO 3, 5 |

### Workbook Exercises

| Exercise | Tasks | Key LOs |
|----------|-------|---------|
| Exercise 1 | 1.1-1.3: AR/MA process definitions and signatures | LO 1 |
| Exercise 2 | 2.1: Differencing; 2.2-2.3: ARIMA fitting | LO 2, 4 |
| Exercise 3 | 3.1-3.2: Residual diagnostics | LO 5 |
| Exercise 4 | 4.1: AIC/BIC comparison; 4.2-4.3: Forecasting | LO 3, 6 |
| Problem 1 | Extended AR/MA identification | LO 1 |
| Problem 2 | Forecasting with prediction intervals | LO 3 |

---

## Common Misconceptions

**1. "AR models use past values; MA models use past values too, just
differently."**

AR models use past *observed values*. MA models use past *shocks*
(innovations, prediction errors). This is a fundamental distinction.
Past observed values are directly available; past shocks must be
estimated. The two model types capture different dependence mechanisms.

*Probe:* "In an AR(1) model, tomorrow depends on today's observed
value. In an MA(1) model, tomorrow depends on today's surprise. Can
you explain what 'today's surprise' means operationally?"

**2. "More parameters always give a better model."**

More parameters always improve *fit* (reduce residual variance) but
risk *overfitting*. AIC and BIC penalize complexity: AIC balances fit
and parsimony; BIC penalizes more heavily, preferring simpler models.
The best model captures the structure without fitting the noise.

*Probe:* "If an ARMA(3,3) has lower residual variance than an AR(2),
does that mean it's a better model? What else should you check?"

**3. "Differencing is just a transformation to make the math work."**

Differencing has a substantive interpretation: it converts levels to
changes. An ARIMA(0,1,0) is a random walk; its differences are white
noise. An ARIMA(1,1,0) says the changes follow an AR(1). Whether
differencing is appropriate depends on whether the scientific question
concerns levels or changes. The global temperature trend is the signal
if you care about levels; differencing removes it.

*Probe:* "If I difference the global temperature series, what question
am I now analyzing? Is that the question I actually want to answer?"

**4. "The prediction interval is just the forecast plus or minus a
fixed amount."**

Prediction intervals *widen with horizon*. One-step-ahead uncertainty
is just the innovation variance. Multi-step uncertainty accumulates as
the psi-weights compound. For ARIMA models, long-horizon intervals
approach the unconditional variance. You cannot predict arbitrarily
far ahead with precision.

*Probe:* "Why does the prediction fan get wider as you forecast further
out? What would it mean if it didn't?"

**5. "The ACF/PACF patterns will clearly tell me what model to fit."**

In theory, AR shows PACF cutoff; MA shows ACF cutoff. In practice,
sample ACF and PACF are noisy. Patterns may be ambiguous, especially
for ARMA processes where both decay. The practical approach: fit
several candidates, compare via AIC/BIC, check residuals.

*Probe:* "Both the ACF and PACF seem to decay for this series. What
does that suggest, and what would you try?"

**6. "Exponential smoothing is just a heuristic, not a real
statistical method."**

Exponential smoothing is the *optimal* forecast (minimum mean squared
prediction error) for the IMA(1,1) model. The smoothing parameter
alpha equals 1 minus the MA coefficient. Holt-Winters extensions
correspond to specific state-space models. The connection between
a widely-used practical method and rigorous statistical theory is one
of the chapter's key insights.

*Probe:* "If exponential smoothing is optimal for IMA(1,1), what
would have to be true about the data for it to be a good forecasting
method?"

---

## Scaffolding Strategies

**When the student cannot distinguish AR from MA:**
Side-by-side simulation is effective. Generate an AR(1) with phi=0.7
and an MA(1) with theta=0.7. Plot both ACFs and PACFs. The AR shows
gradual ACF decay with PACF cutoff at lag 1; the MA shows the
opposite. Ask: "Which one has 'infinite memory' (correlation at all
lags) and which has 'finite memory' (correlation only at lag 1)?"

**When the student struggles with operator notation:**
The back-shift notation is compact but initially opaque. Start by
expanding: phi(B)X(t) = X(t) - phi_1 X(t-1) - phi_2 X(t-2). Ask the
student to write out the expanded form before using the shorthand.
The polynomial notation is a bookkeeping device; the concept is just
"weighted combination of past values."

**When the student is overwhelmed by SARIMA notation:**
Break it into pieces. ARIMA(1,1,1)x(0,1,1)_12 means: non-seasonal
part is AR(1), one regular difference, MA(1); seasonal part is no
seasonal AR, one seasonal difference, seasonal MA(1) at period 12.
Have the student identify each component separately before combining.

**When the student's residuals show structure:**
This is the Box-Jenkins diagnostic step. If residual ACF shows
significant values, the model has not captured all the dependence.
Guide the student: "At which lags is the residual ACF significant?
What does that suggest about additional AR or MA terms?" This is
where the iterative workflow becomes concrete.

**When the student confuses the "I" in ARIMA with integration:**
The "I" stands for "integrated" because if the *differences* follow
an ARMA process, then the original series is the cumulative sum
(integral) of that process. A random walk is ARIMA(0,1,0): its
differences are white noise, and the series itself is the running
sum of white noise. Ask: "If Y(t) = X(t) - X(t-1) is ARMA, what
operation recovers X(t) from Y(t)?"

**When the student is unsure which R function to use:**
The primary interfaces are `stats::arima()` for fitting ARIMA models
and `astsa::sarima()` for SARIMA with built-in diagnostic plots.
For simulation, `stats::arima.sim()` generates from a specified
model. For forecasting, `predict()` on a fitted model or
`astsa::sarima.for()`. Guide the student to the function that
matches their current task rather than presenting all options at once.

---

## Key Connections

**Backward to earlier chapters:**
- ACF and PACF diagnostics (Ch 12): The entire model identification
  strategy rests on reading ACF/PACF patterns.
- Stationarity and differencing (Ch 12): Non-stationary series require
  differencing before ARMA modeling.
- Back-shift operator (Ch 12): The operator notation for AR and MA
  polynomials.
- Simulation (Ch 4): Generating from known models to verify theory
  (Exercises 1-2, 6).

**Forward to subsequent chapters:**
- Ch 14 (Frequency Domain): The AR(2) spectral peak reveals periodic
  structure that the time-domain model captures but doesn't explicitly
  display. ARMA spectra connect parameters to frequency features.

**If the student asks about topics not covered in this chapter:**
- Neural forecasting (LSTMs, transformers): Acknowledge as powerful
  extensions for large datasets and complex patterns, but note that
  ARIMA models remain competitive for many univariate series and
  provide interpretable parameters.
- State-space models: Mention as a unifying framework that subsumes
  ARIMA and exponential smoothing, but beyond scope.
- Multivariate time series (VAR): Acknowledge the extension from
  univariate to vector processes; the concept parallels multivariate
  regression.

---

## Terminology

| Term | Definition |
|------|-----------|
| AR(p) | Autoregressive model of order p: current value depends on p past values |
| MA(q) | Moving average model of order q: current value depends on q past shocks |
| ARMA(p,q) | Combined autoregressive and moving average model |
| ARIMA(p,d,q) | ARMA applied to d-th differenced series |
| SARIMA | Seasonal ARIMA with both non-seasonal and seasonal components |
| Causality | AR polynomial roots outside unit circle; process is a convergent sum of past shocks |
| Invertibility | MA polynomial roots outside unit circle; shocks recoverable from observed values |
| AIC | Akaike Information Criterion: -2 log L + 2k |
| BIC | Bayesian Information Criterion: -2 log L + k log T |
| Ljung-Box test | Tests whether residual autocorrelations are jointly zero |
| Prediction interval | Range for future values that widens with forecast horizon |
| Exponential smoothing | Recursive forecasting method; optimal for IMA(1,1) |

---

## Companion Guidance

This chapter is model-heavy. Students need to develop both the
theoretical understanding (why does PACF cutoff identify AR order?)
and the practical skill (fit a model in R, check residuals, iterate).
Balance both: if a student can fit models but cannot explain why
they chose that model, push on the diagnostic reasoning. If they
understand the theory but struggle with the code, focus on the
`stats::arima()` and `astsa::sarima()` interfaces.

The Box-Jenkins workflow is an excellent case study in iterative
modeling discipline. Emphasize that a first model is rarely adequate;
the diagnostic checking step is where learning happens.

Students who enjoy mathematical structure may want to explore the
psi-weight representation, the connection between AR and MA via
polynomial inversion, or the proof that exponential smoothing is
optimal for IMA(1,1). Encourage these explorations; the algebra
deepens understanding of why the models work.
