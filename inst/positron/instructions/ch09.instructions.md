---
name: "Chapter 9: Linear Discriminant Analysis"
description: >
  Content knowledge and tutoring guidance for the EDA Companion when
  the student is working on Chapter 9 materials.
applyTo: "**/ch09*"
---

<!-- Last updated: 2026-02-25 -->

<!-- Source of truth: eda4ml-positron/ch-instructions/ch09.instructions.md -->

# Chapter 9: Linear Discriminant Analysis

## Chapter Scope and Position

Chapter 9 completes the Part 2 arc by introducing the supervised
counterpart to PCA. Where PCA asks "along which directions do the
features vary most?", LDA asks "along which directions are the
classes best separated?" The chapter develops LDA from two converging
perspectives: Bayesian decision theory (Gaussian assumptions yield
linear discriminant functions) and Fisher's geometric criterion
(maximize between-class variance relative to within-class variance).

**The intellectual arc:** Chapter 7 established projection onto
model-specified subspaces (regression). Chapter 8 found data-derived
subspaces that maximize variance (PCA). Chapter 9 finds subspaces
that maximize class separation (LDA). All three are orthogonal
projection problems; they differ in what "optimal" means.

**Core insight:** With K classes, LDA produces at most K-1
discriminant directions, regardless of the number of features d.
For binary classification, there is exactly one discriminant direction.
This dramatic dimension reduction follows from the rank of the
between-class covariance matrix B, which has at most K-1 nonzero
eigenvalues.

**Pedagogical approach:** The chapter begins with a one-dimensional
bookstore example (classify books as general or technical by price)
to introduce loss, risk, and the Bayes decision rule. It then
extends to multiple features and classes, develops Fisher's
geometric perspective, and applies LDA to the Wine Quality data
from Chapter 8. The wine color classification provides continuity:
PCA discovered the red/white separation unsupervised; now LDA
exploits it.

**Two derivations, one answer:** The Bayesian derivation assumes
Gaussian distributions with common covariance and yields
discriminant functions with posterior probabilities. Fisher's
derivation is distribution-free and yields the same directions
from a purely geometric argument. The equivalence is both
theoretically satisfying and practically useful.

**Assumed background:** Chapters 1-8 completed. Orthogonal
projection, eigenvalues, covariance matrices from Chapters 7-8.
The Gaussian distribution from Chapter 4. The confusion matrix is
introduced in this chapter.

**Where it leads:** Part 3 (Text Data) introduces the other LDA
(Latent Dirichlet Allocation). The chapter explicitly flags this
naming collision. The survey of dimension reduction methods
(Section 9.8) positions PCA and LDA within a broader landscape
including t-SNE, UMAP, factor analysis, ICA, and PLS.

---

## Learning Objectives

The student should be able to:

1. Distinguish supervised from unsupervised methods, and dimension
   reduction from classification.
2. Explain how LDA differs from PCA in its objective function.
3. Interpret Fisher's criterion for LDA.
4. Describe the relationship between LDA and logistic regression.
5. Apply LDA in R to classify observations into groups.
6. Evaluate classification accuracy using a confusion matrix.
7. Distinguish between LDA and QDA and identify when each is
   appropriate.

---

## Concept Inventory

### Supervised vs. Unsupervised; Reduction vs. Classification (LO 1)

Unsupervised methods (PCA, clustering) seek structure without
reference to class labels. Supervised methods (LDA, logistic
regression) use labels to guide the analysis. Dimension reduction
(PCA, LDA projections) maps high-dimensional data to fewer
dimensions. Classification (LDA decision rule, logistic regression)
assigns observations to classes. LDA does both: it finds discriminant
directions (dimension reduction) and applies Bayes' rule to classify
(classification).

### LDA vs. PCA Objective Functions (LO 2)

PCA maximizes total variance of projected data. LDA maximizes
between-class variance relative to within-class variance. In
the "sphered" space where within-class covariance equals the
identity, Fisher's LDA reduces to PCA of the class centroids.

The key implication: directions of maximum overall variance may
not separate classes. A high-variance direction could be driven
entirely by within-class spread. LDA penalizes within-class spread
by putting it in the denominator of the Rayleigh quotient.

### Fisher's Criterion (LO 3)

Fisher seeks direction a that maximizes a'Ba / a'Wa, where B is the
between-class covariance (spread of class means around the grand
mean) and W is the within-class covariance (pooled spread of
observations around their class means). The total covariance
decomposes as T = W + B.

The solution is the eigenvector of W^{-1}B corresponding to the
largest eigenvalue. For K = 2 classes, this simplifies to
a proportional to W^{-1}(mu_1 - mu_2): the within-class-adjusted
direction connecting the two class means.

The Rayleigh quotient is scale-invariant: multiplying a by any
constant leaves the ratio unchanged.

### LDA and Logistic Regression (LO 4)

Both produce linear decision boundaries. LDA assumes Gaussian
class-conditional distributions with common covariance; logistic
regression assumes linearity of log-odds without distributional
assumptions on features.

LDA is more efficient under correct assumptions (uses data from
all classes to estimate the shared covariance). Logistic regression
is more robust to violations. In practice, both often yield similar
results. The chapter invites students to compare them in exercises.

### Applying LDA in R (LO 5)

Two interfaces: `MASS::lda()` (classic) and `discrim::discrim_linear()`
via tidymodels. The `scaling` matrix from `MASS::lda()` contains the
discriminant directions (LD1, LD2, ...). The `predict()` function
returns predicted classes and posterior probabilities.

The wine color example demonstrates both the 2-feature case (density,
residual sugar; visualizable decision boundary) and the 11-feature
case (substantial improvement in accuracy).

### Confusion Matrices (LO 6)

A confusion matrix cross-tabulates actual classes against predicted
classes. Off-diagonal entries are misclassifications. Accuracy is
the proportion of correct classifications. The chapter computes
confusion matrices for both the 2-feature and 11-feature wine
color models, showing how additional features reduce error rates.

The wine quality example (3-level quality: low, medium, high) shows
a more challenging classification with substantial overlap between
adjacent quality levels.

### LDA vs. QDA (LO 7)

LDA assumes equal covariance across classes, yielding linear
(hyperplane) boundaries. QDA allows class-specific covariance
matrices, yielding quadratic boundaries. QDA is appropriate when
class variances differ substantially.

The bookstore example illustrates: scenarios (a) and (b) have
equal variance (linear boundary); scenario (c) has unequal variance
(quadratic boundary).

QDA has more parameters to estimate (a separate d-by-d covariance
per class), so it requires more data and is more prone to
overfitting with many features.

---

## Exercise Map

### Book Exercises (from lin-discr.qmd)

| #   | Title / Task           | Key Objectives | Dataset                 |
| --- | ---------------------- | -------------- | ----------------------- |
| 1   | Wine Color LDA         | LO 5, 6        | `eda4mlr::wine_quality` |
| 2   | LDA with K = 3         | LO 5, 6        | Wine quality (3 levels) |
| 3   | Comparing PCA and LDA  | LO 1, 2        | `beans::beans`          |
| 4   | When PCA Fails         | LO 1, 2        | Constructed 2D example  |
| 5   | Fisher's Criterion     | LO 3           | Conceptual              |
| 6   | Equal Covariances      | LO 7           | Conceptual / QDA        |
| 7   | Supervised vs. Unsup.  | LO 1           | Scenarios               |

### Workbook Exercise Map (from objective-exercise-pairs.tsv)

| Exercise | Task | Learning Objective(s)                    |
| -------- | ---- | ---------------------------------------- |
| Ex 1     | 1    | LO 1 (supervised vs. unsupervised)       |
| Ex 1     | 2    | LO 2 (LDA vs. PCA objective)            |
| Ex 1     | 3    | LO 1, 2 (distinction and comparison)    |
| Ex 2     | 1    | LO 3 (Fisher's criterion)               |
| Ex 2     | 2    | LO 3 (Fisher's criterion)               |
| Ex 3     | 1    | LO 5 (apply LDA in R)                   |
| Ex 3     | 2    | LO 5, 6 (application and confusion matrix) |
| Ex 4     | 1    | LO 7 (LDA vs. QDA)                      |
| Ex 4     | 2    | LO 7 (LDA vs. QDA)                      |
| Ex 4     | 3    | LO 7 (LDA vs. QDA)                      |
| Prob 1   | -    | LO 3 (Fisher's criterion)               |
| Prob 2   | -    | LO 2 (LDA vs. PCA)                      |

---

## Common Misconceptions

1. **LDA and PCA find the same directions.** They solve different
   optimization problems. PCA maximizes total variance; LDA
   maximizes the ratio of between-class to within-class variance.
   These coincide only when within-class covariance is spherical
   and class structure aligns with variance structure. Exercise 4
   constructs a 2D counterexample. Probe: "Can you think of a
   dataset where PC1 is perpendicular to LD1?"

2. **LDA requires Gaussian data.** The Bayesian derivation assumes
   Gaussianity. Fisher's derivation does not. LDA often works well
   even when the Gaussian assumption is violated, because the
   Fisher directions depend only on means and covariances, not on
   the full distributional form. Probe: "Fisher's criterion uses
   only W and B. Where do distributional assumptions enter?"

3. **More discriminant directions is better.** With K classes, there
   are at most K-1 nonzero discriminant directions. For binary
   classification, there is exactly one. This is not a limitation;
   it reflects the rank of the between-class covariance. Probe:
   "For K = 2, why does B have rank 1?"

4. **A high accuracy means the model is good.** Accuracy depends on
   class balance. If 90% of wines are white, a model that always
   predicts "white" has 90% accuracy but is useless. The confusion
   matrix reveals per-class performance. Probe: "What does the
   confusion matrix tell you that overall accuracy does not?"

5. **LDA is obsolete because logistic regression is more flexible.**
   LDA provides dimension reduction, posterior probabilities, and
   Fisher's geometric interpretation. It is more efficient than
   logistic regression when assumptions hold. The methods are
   complementary, not competing. Probe: "What does LDA give you
   that logistic regression does not?"

---

## Scaffolding Strategies

### When a student is stuck on supervised vs. unsupervised

- "In PCA, did we use the wine color labels? In LDA, do we?"
- "Clustering (Chapter 3) is unsupervised; it groups without
  labels. LDA is supervised; it uses labels to find separating
  directions. What is the analogous relationship between PCA
  and LDA?"

### When a student is stuck on Fisher's criterion

- "Think of it as a signal-to-noise ratio. Between-class variance
  is the signal (how far apart are the class means?). Within-class
  variance is the noise (how spread out is each class?). Fisher
  maximizes signal relative to noise."
- "Project all observations onto a candidate direction a. Compute
  the between-class spread and within-class spread of the projected
  values. Fisher picks the a that maximizes their ratio."

### When a student is stuck on the rank constraint

- "K class means live in an affine subspace of dimension at most
  K-1 (they are constrained by the grand mean). So B, which
  measures spread of the class means, has rank at most K-1."
- "For K = 2: the two class means define a single direction
  (the line connecting them). B is proportional to the outer
  product of (mu_1 - mu_2), which has rank 1."

### When a student is stuck on Exercise 1 (Wine Color)

- "Start with the 2-feature case (density, residual sugar).
  Plot the data colored by wine color. Where would you draw
  a line to separate the colors?"
- "The LDA decision boundary is a line in 2D. The LD1 direction
  is perpendicular to this boundary. What does the boundary's
  orientation tell you about which features matter?"

### When a student is stuck on Exercise 3 (Comparing PCA and LDA)

- "Run PCA and LDA on the same data. Project onto the first two
  directions from each. Color by class. Which projection separates
  the classes better?"
- "Compute the angle between PC1 and LD1. If it is near 0, the
  methods agree. If it is near 90 degrees, they are finding very
  different structure."

### When a student is stuck on QDA

- "Return to the bookstore example, scenario (c). One class has
  larger variance. What shape is the optimal decision boundary?"
- "LDA uses one covariance matrix for all classes. QDA uses a
  separate covariance matrix per class. What is the cost of
  this flexibility?"

---

## Key Connections

**Backward to Chapters 1-8:**

- The wine data is continuous from Chapter 8: PCA discovered the
  red/white separation unsupervised; now LDA exploits the known
  labels.
- Fisher's criterion echoes the elbow method (Chapter 3): both
  balance signal (between-group structure) against noise
  (within-group spread).
- The Bayes decision rule connects to probability and simulation
  (Chapter 4): prior probabilities, likelihood ratios, and expected
  loss.
- The confusion matrix is this chapter's version of cluster
  evaluation (Chapter 3): comparing algorithmic output to known
  structure.
- Orthogonal projection (Chapter 7) and eigendecomposition
  (Chapter 8) are the shared mathematical tools.

**Forward references:**

- Part 3 introduces the other LDA (Latent Dirichlet Allocation).
  The acronym collision is flagged explicitly: context disambiguates.
- The dimension reduction survey (Section 9.8) positions PCA and
  LDA within a landscape including t-SNE, UMAP, factor analysis,
  ICA, and PLS.
- Cross-validation (mentioned but not developed) appears in later
  ML coursework for honest assessment of classification accuracy.

**If the student asks about topics not yet covered:**

If a student asks about logistic regression in detail, note that the
chapter compares LDA and logistic regression conceptually and invites
an exercise comparison, but does not develop logistic regression's
full machinery. If a student asks about regularized discriminant
analysis, connect to the multicollinearity discussion in Chapter 7:
when d > n, W may be singular, requiring regularization.

---

## Terminology

| Term                     | Definition (per the book)                                                                       |
| ------------------------ | ----------------------------------------------------------------------------------------------- |
| Supervised learning      | Methods that use labeled data (class assignments) to guide analysis                             |
| Unsupervised learning    | Methods that seek structure without labels                                                      |
| Discriminant function    | Linear function of features used to assign observations to classes                              |
| Fisher's criterion       | Maximize a'Ba / a'Wa: between-class variance relative to within-class variance                  |
| Rayleigh quotient        | The ratio a'Ba / a'Wa; scale-invariant                                                          |
| Within-class covariance  | W: pooled covariance of observations about their class means                                    |
| Between-class covariance | B: covariance of class means about the grand mean                                               |
| Discriminant direction   | Eigenvector of W^{-1}B; labeled LD1, LD2, etc.                                                  |
| Prior probability        | pi_k = P(Y = C_k); estimated by class proportion n_k / n                                        |
| Posterior probability    | P(Y = C_k given x); computed from discriminant functions via Bayes' rule                         |
| Confusion matrix         | Cross-tabulation of actual vs. predicted class labels                                            |
| QDA                      | Quadratic Discriminant Analysis; allows class-specific covariance matrices                       |
| Decision boundary        | Set of points where discriminant functions for two classes are equal                             |
| Canonical variates       | Synonym for Fisher's discriminant coordinates (LD1, LD2, ...)                                    |

---

## Companion Guidance for This Chapter

When helping a student with Chapter 9 material:

- **The PCA-LDA contrast is the central lesson.** This chapter's
  deepest value lies in the comparison: same geometric framework,
  different objectives, different results. Exercise 3 (comparing
  PCA and LDA directions on bean data) and Exercise 4 (constructing
  a case where they diverge) are essential.

- **The bookstore example builds intuition.** The one-dimensional
  toy example introduces loss, risk, priors, and the Bayes rule
  in a concrete setting before generalizing to multiple features.
  If a student skips it, redirect: understanding the three
  scenarios (equal/unequal proportions, equal/unequal variance) is
  foundational for understanding when LDA vs. QDA is appropriate.

- **Fisher's criterion is the geometric key.** The Bayesian
  derivation gives the machinery (discriminant functions, posterior
  probabilities). Fisher's derivation gives the intuition
  (maximize signal-to-noise). Both yield the same directions. Help
  the student see the convergence: "Two very different starting
  points arrive at the same answer. Why?"

- **The wine data provides continuity.** PCA (Chapter 8) discovered
  the red/white pattern unsupervised. LDA now formalizes and
  quantifies the separation using labels. This progression from
  discovery to confirmation is a model for real data analysis.

- **The two LDAs are genuinely confusing.** Linear Discriminant
  Analysis (this chapter) vs. Latent Dirichlet Allocation
  (Chapter 11). When the student encounters this, name the
  collision directly rather than hoping context suffices.

- **The confusion matrix deserves attention.** Students sometimes
  report only overall accuracy. Push for per-class analysis: which
  classes are confused? Is the confusion symmetric? Does class
  imbalance matter? The wine quality 3-class example (most wines
  classified as "medium") illustrates these issues.

- **Celebrate mathematical exploration.** A student who proves that
  B has rank at most K-1, derives the Fisher direction for K = 2,
  or verifies the equivalence of the Bayesian and Fisher approaches
  is working at the right level for this chapter.
