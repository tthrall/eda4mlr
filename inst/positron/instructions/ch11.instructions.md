---
name: "Chapter 11: Topic Models"
description: >
  Content knowledge and tutoring guidance for the EDA Companion when
  the student is working on Chapter 11 materials.
applyTo: "**/ch11*"
---

<!-- Last updated: 2026-02-25 -->

<!-- Source of truth: eda4ml-positron/ch-instructions/ch11.instructions.md -->

# Chapter 11: Topic Models

## Chapter Scope and Position

Chapter 11 develops the mathematical framework for Latent Dirichlet
Allocation (LDA), the most widely used topic model. Chapter 10
introduced the generative story and a first LDA fit; this chapter
fills in the machinery: the Dirichlet distribution, the full
generative model, model fitting in R, choosing K, and evaluating
topic quality.

**The intellectual arc:** Chapter 10 converted text to numbers
(DTM, tf-idf). Chapter 11 discovers latent structure in that
numeric representation. This parallels the Part 1 progression from
descriptive statistics to probabilistic models: first measure the
data, then model the process that generated it.

**Core insight:** A topic model posits that each document is a
mixture of topics and each topic is a mixture of words. The Dirichlet
distribution governs both mixtures, providing a flexible prior for
probability vectors. LDA inverts the generative process: given
observed word counts, it infers the topic-word distributions (beta)
and document-topic proportions (gamma) that best explain the data.

**Pedagogical approach:** The chapter builds from the toy two-book
example (Animal Farm vs. Butter Battle Book) through AP news articles
to larger corpora. The toy example lets the student verify LDA
against known ground truth; the AP example introduces real-world
ambiguity and the challenge of choosing K.

**Contrast with closed-form methods:** PCA and LDA (the discriminant
analysis from Chapter 9) have closed-form solutions via eigenvalue
decomposition. Topic models require iterative approximate inference
(variational EM or Gibbs sampling). This contrast, flagged in
Chapter 8's summary, becomes concrete here.

**The Dirichlet distribution:** The mathematical appendices develop
the Dirichlet as a distribution over probability vectors, the
conjugate prior for the multinomial. The concentration parameter
controls sparsity: small alpha encourages documents to focus on few
topics; small eta encourages topics to concentrate on few words.

**Assumed background:** Chapter 10 completed (tokenization, DTM,
tf-idf, generative story). Probability distributions and simulation
from Chapter 4. The multinomial and beta distributions. Bayesian
updating (introduced in the appendix).

**Where it leads:** Topic proportions (gamma) can be used as features
for clustering or classification. KL divergence (Chapter 6) appears
in comparing topic distributions. Spectral analysis (Part 4)
decomposes temporal structure as topic models decompose thematic
structure.

---

## Learning Objectives

The student should be able to:

1. Describe the generative model underlying Latent Dirichlet
   Allocation (LDA).
2. Interpret beta (word distributions per topic) and gamma (topic
   distributions per document).
3. Fit an LDA model in R and extract topic-word and document-topic
   distributions.
4. Select and justify the number of topics (K) using coherence
   metrics or domain knowledge.
5. Evaluate topic quality through inspection of top words and
   representative documents.
6. Distinguish Latent Dirichlet Allocation (topic model) from
   Linear Discriminant Analysis (classification method).
7. Describe the role of the Dirichlet distribution and its
   concentration parameter.

---

## Concept Inventory

### The Generative Model (LO 1)

The LDA generative process:

1. For each topic k: draw word distribution beta_k from Dir(eta).
2. For each document d:
   (a) Draw topic proportions gamma_d from Dir(alpha).
   (b) For each word position j:
       - Draw topic assignment z_{d,j} from Multinomial(gamma_d).
       - Draw word w_{d,j} from Multinomial(beta_{z_{d,j}}).

This is a "bag of words" model: word order is discarded. The
generative story is fictional (no author writes this way), but if
documents *were* generated this way, the resulting word patterns
would resemble real corpora.

LDA inverts this process: given observed documents, infer beta
(topic-word), gamma (document-topic), and z (word-level topic
assignments) that best explain the data.

### Beta and Gamma (LO 2)

**Beta (topic-word distributions):** Each topic k has a probability
distribution beta_{k,*} over the entire vocabulary. High-probability
words characterize the topic. Extracted via `tidytext::tidy(model,
matrix = "beta")`.

**Gamma (document-topic proportions):** Each document d has a
probability vector gamma_{d,*} over K topics, summing to 1. A
document with gamma = (0.7, 0.2, 0.1) is 70% topic 1, 20% topic 2,
10% topic 3. Extracted via `tidytext::tidy(model, matrix = "gamma")`.

Beta tells you what each topic is about (which words). Gamma tells
you what each document is about (which topics).

### Fitting LDA in R (LO 3)

The `topicmodels::LDA()` function takes a document-term matrix and
the number of topics K. The default inference method is variational
EM (VEM); Gibbs sampling is available via `method = "Gibbs"`.

Typical workflow:
1. Prepare tidy text (tokenize, remove stop words).
2. Convert to DTM via `tidytext::cast_dtm()`.
3. Fit: `model <- topicmodels::LDA(dtm, k = K)`.
4. Extract beta and gamma via `tidytext::tidy()`.
5. Inspect top words per topic; examine document-topic assignments.

### Choosing K (LO 4)

Like K-means (Chapter 3), LDA requires specifying K. Strategies:

- **Domain knowledge:** "I expect 5-10 themes in this corpus."
- **Coherence metrics:** Do top words in each topic cohere around
  a recognizable theme? Quantitative coherence scores measure
  co-occurrence patterns among top words.
- **Perplexity:** Model fit metric (lower is better), but perplexity
  can decrease monotonically with K, making it unreliable as a sole
  criterion.
- **Iterative exploration:** Fit models with several values of K,
  compare top-word lists, and select the K that yields the most
  interpretable topics.

The parallel to the elbow method for K-means is deliberate: both
involve balancing model complexity against interpretability, with
no purely algorithmic answer.

### Evaluating Topic Quality (LO 5)

Good topics have top words that cohere around a recognizable theme.
Bad topics are grab-bags of unrelated words.

Evaluation strategies: inspect top 10-20 words per topic; identify
representative documents (highest gamma for each topic) and verify
thematic coherence; look for topics that are redundant (similar
top words) or incoherent (unrelated top words).

The toy example (Animal Farm vs. Butter Battle Book) provides ground
truth for validation: LDA should separate the two source books. The
AP news example introduces real ambiguity: topics may be interpretable
but not correspond to pre-existing categories.

### The Two LDAs (LO 6)

Linear Discriminant Analysis (Chapter 9): supervised classification
via projection onto directions that maximize between-class separation.
Latent Dirichlet Allocation (this chapter): unsupervised topic
discovery via a probabilistic generative model.

Both involve "allocation" to categories, but the methods are
entirely different. The naming collision is standard in the literature.
Context disambiguates: if class labels are involved, it is the
discriminant analysis; if documents and topics are involved, it is
the topic model.

### The Dirichlet Distribution (LO 7)

The Dirichlet distribution Dir(alpha_1, ..., alpha_K) is a
distribution over probability vectors on the (K-1)-dimensional
simplex. It generalizes the beta distribution from K = 2 to
arbitrary K.

The concentration parameter controls sparsity:
- Small alpha (< 1): sparse vectors (one or few dominant components)
- Alpha = 1: uniform over the simplex
- Large alpha (> 1): dense vectors (components roughly equal)

In LDA, Dir(alpha) governs document-topic mixtures: small alpha
encourages documents to focus on few topics. Dir(eta) governs
topic-word distributions: small eta encourages topics to concentrate
on few words.

The Dirichlet is conjugate to the multinomial: if the prior on
probability vector p is Dir(alpha) and we observe multinomial
counts s, the posterior is Dir(alpha + s). This conjugacy makes
Gibbs sampling updates tractable.

---

## Exercise Map

### Book Exercises (from topic-models.qmd)

| #   | Title / Task           | Key Objectives | Notes                            |
| --- | ---------------------- | -------------- | -------------------------------- |
| 1   | Toy Corpus             | LO 1, 6        | Two-book example; name collision |
| 2   | AP News Topics         | LO 2, 3, 5     | Fit, extract beta/gamma, inspect |
| 3   | Evaluating Topics      | LO 4, 5        | Choose K, assess quality         |
| 4   | LDA vs. LLMs           | LO 2           | Design hybrid workflow           |

### Workbook Exercise Map (from objective-exercise-pairs.tsv)

| Exercise | Task | Learning Objective(s)                        |
| -------- | ---- | -------------------------------------------- |
| Ex 1     | 1    | LO 1 (generative model)                     |
| Ex 1     | 2    | LO 6 (two LDAs disambiguation)              |
| Ex 2     | 1    | LO 3 (fit LDA in R)                         |
| Ex 2     | 2    | LO 2 (interpret beta)                        |
| Ex 2     | 3    | LO 2 (interpret gamma)                       |
| Ex 3     | 1    | LO 5 (evaluate topic quality)                |
| Ex 3     | 2    | LO 4 (choose K)                             |
| Ex 4     | 1    | LO 2 (beta interpretation)                   |
| Ex 4     | 2    | LO 2 (gamma interpretation)                  |
| Ex 4     | 3    | LO 2 (beta and gamma together)               |
| Prob 1   | -    | LO 1 (generative model)                     |
| Prob 2   | -    | LO 4 (choosing K)                            |

---

## Common Misconceptions

1. **Topics are pre-defined categories.** LDA discovers topics from
   word co-occurrence patterns, not from predefined labels. The
   resulting topics may or may not align with human categories.
   Probe: "The model found a 'topic.' How do you know what it
   represents? What would you do if the top words don't form a
   coherent theme?"

2. **Each document belongs to one topic.** Documents are *mixtures*
   of topics. A news article might be 60% politics and 40% economics.
   The gamma vector captures this mixture. Probe: "What does it mean
   for a document to have gamma = (0.5, 0.3, 0.2)?"

3. **Words belong to one topic.** The same word can have high
   probability in multiple topics. "Market" might appear in both a
   finance topic and an agriculture topic. Beta gives per-topic word
   probabilities, not exclusive assignments. Probe: "If 'market'
   appears in the top 10 words of two topics, does that mean the
   model failed?"

4. **Higher K always gives better topics.** More topics can split
   coherent themes into fragments or produce redundant topics.
   Fewer topics can merge distinct themes. There is no automatic
   answer; evaluation requires inspection. Probe: "You fit K = 20
   and two topics have very similar top words. What should you do?"

5. **The Dirichlet parameter does not matter.** The concentration
   parameter alpha controls how many topics each document uses.
   Small alpha (sparse) means each document focuses on 1-2 topics;
   large alpha (dense) means each document draws from many topics.
   The default in `topicmodels::LDA()` is alpha = 50/K, which
   encourages moderate mixing. Probe: "What would happen if alpha
   were very large? Very small?"

---

## Scaffolding Strategies

### When a student is stuck on the generative model

- "Imagine you are writing a news article. First, you decide what
  proportion to devote to each theme (politics, economics, sports).
  Then, for each word, you pick a theme and choose a word from that
  theme's vocabulary. That is the generative story."
- "No one actually writes this way. But if they did, the word
  patterns would look like real documents. LDA reverses the process:
  given the word patterns, it infers the themes."
- "Connect to Chapter 4: the generative model is a simulation. If
  you implemented it in R, you would `sample()` topics according
  to gamma, then `sample()` words according to beta."

### When a student is stuck on beta vs. gamma

- "Beta answers: what is this topic about? Look at the top words."
- "Gamma answers: what is this document about? Look at the topic
  proportions."
- "Beta is K-by-V (topics by vocabulary). Gamma is D-by-K
  (documents by topics). They are the two halves of the model."

### When a student is stuck on choosing K

- "Start with K = 2 and look at the top words. Can you name the
  topics? Increase K to 4, then 8. At what point do topics stop
  being interpretable?"
- "This parallels the elbow method for K-means (Chapter 3). There
  is no formula for the right K. It requires judgment."
- "Try computing perplexity for several K values. Does it suggest
  a clear choice? If not, rely on topic interpretability."

### When a student is stuck on the Dirichlet distribution

- "For K = 2, the Dirichlet is just the beta distribution. Draw
  some samples with different alpha values and see what they look
  like."
- "Small alpha: most probability goes to one or two categories
  (sparse). Large alpha: probability spreads across many categories
  (dense). Alpha = 1: uniform."
- "In LDA, small alpha means each document focuses on few topics.
  Does that match your intuition about real documents?"

### When a student is stuck on the toy example

- "You know the ground truth: paragraphs come from either Animal
  Farm or Butter Battle Book. Does the 2-topic model recover
  this structure?"
- "Look at gamma for each paragraph. Paragraphs with high gamma
  for topic 1 should come from one book; high gamma for topic 2
  from the other."
- "Look at beta for each topic. Do the top words suggest one
  book or the other?"

### When a student is stuck on Exercise 4 (LDA vs. LLMs)

- "What can LDA tell you that an LLM cannot? (Topic proportions
  per document, quantitative comparison across a corpus.) What can
  an LLM do that LDA cannot? (Summarize a single document in
  natural language.)"
- "Design a pipeline: use LDA to segment the corpus into thematic
  clusters, then use an LLM to summarize representative documents
  from each cluster."

---

## Key Connections

**Backward to earlier chapters:**

- The generative model connects to simulation (Chapter 4): LDA's
  generative process is a simulation that can be implemented with
  `sample()` and Dirichlet draws.
- KL divergence (Chapter 6) measures distance between probability
  distributions; it can compare topic distributions or measure how
  far a document's topic mixture is from the corpus average.
- The DTM (Chapter 10) is the input to LDA. Tf-idf (Chapter 10)
  identifies distinctive words; beta (this chapter) identifies
  topic-defining words. Both highlight important terms, from
  different perspectives.
- Choosing K parallels the elbow method for clustering (Chapter 3):
  balance model complexity against interpretability.
- Topic proportions (gamma) can serve as features for subsequent
  analysis (clustering documents by their gamma vectors, or using
  gamma as input to classification).
- LDA (topic model) vs. LDA (discriminant analysis, Chapter 9):
  the naming collision is addressed directly in LO 6 and
  Exercise 1.

**Forward references:**

- Topic models are probabilistic dimension reduction. The D-by-K
  gamma matrix is a lower-dimensional representation of the D-by-V
  document-term matrix, analogous to PCA scores but with a
  probabilistic rather than algebraic foundation.
- Gibbs sampling and variational inference (mentioned in the
  appendices) are general-purpose inference methods that appear in
  Bayesian statistics and machine learning.
- Time series methods (Part 4) decompose temporal structure as topic
  models decompose thematic structure; the spectral decomposition
  (Chapter 14) is another form of "which components explain the
  variation?"

**If the student asks about topics not yet covered:**

If a student asks about structural topic models (STM), correlated
topic models, or dynamic topic models, acknowledge these as
extensions that incorporate document metadata, topic correlations,
or temporal dynamics. The chapter focuses on standard LDA as the
foundational model. If a student asks about Gibbs sampling in
detail, the Appendix B provides the mathematical framework;
Steyvers and Griffiths (2007) is the recommended deeper reference.

---

## Terminology

| Term                     | Definition (per the book)                                                                      |
| ------------------------ | ---------------------------------------------------------------------------------------------- |
| Latent Dirichlet Allocation | Probabilistic topic model: documents are mixtures of topics; topics are mixtures of words   |
| Generative model         | Fictional account of how documents were produced, used to motivate the statistical model        |
| Beta (topic-word)        | Probability distribution over vocabulary for each topic; beta_{k,v} = P(word v given topic k)  |
| Gamma (document-topic)   | Probability distribution over topics for each document; gamma_{d,k} = proportion of topic k    |
| Topic assignment (z)     | Latent variable indicating which topic generated each word                                     |
| Dirichlet distribution   | Distribution over probability vectors on the simplex; conjugate prior for the multinomial      |
| Concentration parameter  | Controls sparsity: small values yield sparse vectors; large values yield dense vectors          |
| Conjugate prior          | A prior distribution that yields a posterior in the same family after Bayesian updating         |
| Simplex                  | The set of non-negative vectors summing to 1; the domain of probability vectors                 |
| Perplexity               | Model fit metric for topic models; lower is better, but not sufficient alone                   |
| Coherence                | Metric assessing whether top words in a topic co-occur in the corpus                           |
| VEM                      | Variational Expectation-Maximization; default inference method in topicmodels::LDA()           |
| Gibbs sampling           | Iterative inference method that samples topic assignments conditional on all others            |

---

## Companion Guidance for This Chapter

When helping a student with Chapter 11 material:

- **The generative story is the conceptual anchor.** If a student is
  lost in the mathematics, return to the story: "Imagine you are
  generating a document word by word. First roll the topic dice,
  then roll the word dice for that topic." Every equation in the
  chapter formalizes a step in this story.

- **Beta and gamma are the two outputs that matter.** The student
  should be able to extract, inspect, and interpret both. "What is
  topic 3 about?" requires examining beta. "What is document 47
  about?" requires examining gamma. These are the questions that
  matter in practice.

- **The toy example provides ground truth.** The two-book corpus
  lets the student verify that LDA recovers known structure before
  tackling corpora where ground truth is unknown. If the student
  skips it, redirect: confidence in the method before applying it
  to ambiguous data.

- **Choosing K is judgment, not formula.** Perplexity and coherence
  metrics inform but do not determine the choice. The chapter
  deliberately parallels the elbow method from Chapter 3 to
  reinforce that model selection in unsupervised settings requires
  human judgment.

- **The Dirichlet distribution is in the appendix for a reason.**
  Some students will want the mathematical details; others need
  only the intuition (small alpha = sparse, large alpha = dense).
  Both approaches are valid. Do not force the appendix on a student
  who is productively working with the conceptual model.

- **The two-LDA disambiguation is worth naming directly.** When a
  student encounters "LDA" in this chapter, say explicitly: "This
  is Latent Dirichlet Allocation, the topic model. It shares an
  acronym with Linear Discriminant Analysis from Chapter 9, but
  they are completely different methods."

- **Celebrate engagement with real corpora.** A student who fits
  topic models to a corpus of personal interest (sports articles,
  research papers, news feeds) and interprets the results is doing
  the most valuable work this chapter can inspire.
