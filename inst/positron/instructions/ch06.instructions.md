---
name: "Chapter 6: Information Theory"
description: >
  Content knowledge and tutoring guidance for the EDA Companion when
  the student is working on Chapter 6 materials.
applyTo: "**/ch06*"
---

<!-- Last updated: 2026-02-25 -->

<!-- Source of truth: eda4ml-positron/ch-instructions/ch06.instructions.md -->

# Chapter 6: Information Theory

## Chapter Scope and Position

Chapter 6 is the coda to Part 1 (Foundations of EDA). It introduces
information theory as an alternative lens for understanding
relationships among variables, complementing the geometric perspective
of Chapters 1-3. The chapter is self-contained: subsequent chapters do
not explicitly build on information-theoretic foundations, but students
who pursue specialized ML studies will encounter these ideas repeatedly.

**The intellectual arc:** Chapters 1-5 approached data analysis from
geometric and statistical perspectives (scatter plots, regression lines,
distances, sampling distributions). Chapter 6 introduces a fundamentally
different framework: quantifying uncertainty and the information gained
when uncertainty is resolved. This gives the student vocabulary for
concepts (information gain, cross-entropy loss, KL divergence) that
pervade the ML literature.

**Pedagogical approach:** The chapter builds from a concrete game
(Twenty Questions with lettered tickets) to the formal definition of
entropy, then extends to joint distributions, mutual information, and
KL divergence. The game metaphor makes abstract definitions concrete:
entropy measures the average number of yes-no questions needed to
identify a randomly drawn ticket.

**Coda character:** The chapter explicitly positions itself as investing
in conceptual foundations that will pay dividends later, not as material
needed for the immediate next chapters. This framing should be
communicated to students who feel the chapter is disconnected from
the rest of Part 1.

**Assumed background:** Chapters 1-5 completed. Probability
distributions (Chapter 4). The chi-squared test and statistical
independence (Chapter 2). Logarithms (base 2 and natural).

**Where it leads:** Decision tree splitting (information gain) appears
in ensemble methods. Cross-entropy loss is the standard classification
loss function. KL divergence appears in variational inference (VAEs)
and model comparison. Mutual information is used for feature selection.
These connections are described in the chapter but not developed until
the student encounters them in later courses.

---

## Learning Objectives

The student should be able to:

1. Define, describe, and calculate entropy for finite probability
   distributions.
2. Define and describe mutual information.
3. Define and describe KL divergence.
4. Describe how entropy is related to decision tree splitting.
5. Give an example in which mutual information measures a nonlinear
   relationship that correlation does not measure.
6. Describe the relationship between cross-entropy and KL divergence.
7. Explain why the joint entropy of independent variables equals the
   sum of their individual entropies.

---

## Concept Inventory

### Entropy (LO 1)

For a finite probability distribution p = (p_1, ..., p_K):

H(p) = -Σ p_k × log₂(p_k)

Measured in bits (binary digits) when using log base 2. Represents
the average number of yes-no questions needed to identify a randomly
drawn value under an optimal questioning strategy.

**The ticket-box examples build intuition:**

- Box 1: {A, A, A, A} → H = 0 (no uncertainty)
- Box 2: {A, A, B, B} → H = 1 (one question needed)
- Box 3: {A, B, C, D} → H = 2 (two questions needed)
- Box 4: {A, A, B, C} → H = 3/2 (intermediate uncertainty)

Key properties: H ≥ 0. H = 0 only when one outcome has probability 1.
H is maximized when all outcomes are equally likely (uniform
distribution). For K equally likely outcomes, H = log₂(K).

The optimal strategy is binary search: partition the possible values
into two subsets of nearly equal probability, then identify which
subset contains the drawn value. The maximum number of questions for
K distinct values is the smallest integer ≥ log₂(K).

### Joint Entropy and Independence (LO 7)

For independent random variables (X, Y):

H(X, Y) = H(X) + H(Y)

This follows algebraically from the factorization of the joint
probability: P(X=x, Y=y) = P(X=x) × P(Y=y). The proof in the chapter
uses the property that log of a product equals the sum of logs.

**Ticket-box example:** If letters {A, A, B, C} and numbers {1, 2}
are independent, the contestant can determine the letter (3/2 questions
on average) and then the number (1 question), totaling 5/2 questions.

When (X, Y) are dependent, knowing one variable can reduce the
uncertainty about the other, so H(X, Y) ≤ H(X) + H(Y).

### Mutual Information (LOs 2, 5)

MI(X, Y) = H(X) + H(Y) - H(X, Y)

Mutual information measures the reduction in entropy when moving from
independence to the actual joint distribution. It quantifies how much
knowing one variable tells you about the other.

Key properties: MI ≥ 0. MI = 0 if and only if X and Y are independent.
Unlike correlation, MI captures nonlinear relationships.

**The Venn diagram visualization:** H(X) and H(Y) as overlapping
circles. The overlap is MI(X, Y). The non-overlapping portions are
the conditional entropies H(X|Y) and H(Y|X).

**Connection to correlation (LO 5):** Correlation measures *linear*
association only. Mutual information measures *any* statistical
dependence. A pair (X, Y) where Y = X² and X is symmetric around 0
has correlation zero but positive MI. This is a concrete example of
where MI succeeds and correlation fails.

### KL Divergence (LO 3)

KL(P || Q) = Σ P(x) × log₂(P(x) / Q(x))

KL divergence measures the "cost" of using an incorrect distribution
Q when the true distribution is P. It represents the additional
average number of questions needed by a contestant who optimizes
for Q when the actual box contains distribution P.

Key properties: KL(P || Q) ≥ 0. KL(P || Q) = 0 if and only if P = Q.
KL is *not* symmetric: KL(P || Q) ≠ KL(Q || P) in general. Therefore
KL is not a distance metric, though it is sometimes called a
"divergence."

**The chapter's example:** A contestant optimized for the independent
box {A₁, A₁, B₁, C₁, A₂, A₂, B₂, C₂} but the actual box is the
dependent version {A₁, A₂, B₁, C₂}. The misinformation costs an
extra 1/2 question on average. The KL divergence is 1/2.

### Cross-Entropy and Its Relationship to KL Divergence (LO 6)

Cross-entropy: H(P, Q) = -Σ P(x) × log₂(Q(x))

This equals H(P) + KL(P || Q). Since H(P) is fixed for a given true
distribution, minimizing cross-entropy is equivalent to minimizing
KL divergence.

In ML classification: the true distribution is one-hot (all probability
on the correct class); the model's predicted distribution is Q.
Cross-entropy loss measures how far Q is from the true label
distribution.

### ML Connections (LO 4)

- **Decision trees:** Feature selection by maximizing information gain
  (= reduction in entropy from splitting on a feature). Random forests
  and gradient-boosted trees inherit this.
- **Cross-entropy loss:** Standard loss function for classification.
- **Feature selection:** MI as a model-agnostic measure of feature-target
  dependence, capturing nonlinear relationships that correlation misses.
- **Variational inference:** VAEs minimize KL divergence between
  approximate and true posterior.
- **Model comparison:** KL divergence and Jensen-Shannon divergence
  for comparing probability distributions (topic modeling, GANs).

---

## Exercise Map

### Book Exercises (from info-theory.qmd)

| #   | Title                     | Key Objectives | Notes                                              |
| --- | ------------------------- | -------------- | -------------------------------------------------- |
| 1   | Entropy, Discrete Uniform | LO 1           | Calculate H for K equally likely outcomes          |
| 2   | Entropy, UCB Admissions   | LO 1, 2        | H for decision, joint H, MI for sex and decision   |
| 3   | Cross-Entropy Intuition   | LO 6           | Compare loss for different predicted distributions |
| 4   | MI for Feature Selection  | LO 2, 5        | Compare MI to correlation on real/simulated data   |

### Workbook Exercise Map (from objective-exercise-pairs.tsv)

| Exercise | Task | Learning Objective(s)             |
| -------- | ---- | --------------------------------- |
| Ex 1     | 1    | LO 1 (entropy calculation)        |
| Ex 1     | 2    | LO 1 (entropy calculation)        |
| Ex 1     | 3    | LO 4 (entropy and decision trees) |
| Ex 2     | 1    | LO 2 (mutual information)         |
| Ex 2     | 2    | LO 5 (MI vs. correlation)         |
| Ex 3     | 1    | LO 3 (KL divergence)              |
| Ex 3     | 2    | LO 6 (cross-entropy)              |
| Ex 4     | 1    | LO 2 (mutual information)         |
| Prob 1   | -    | LO 1 (entropy)                    |
| Prob 2   | -    | LO 3 (KL divergence)              |

---

## Common Misconceptions

1. **Entropy measures disorder in the colloquial sense.** In
   information theory, entropy measures uncertainty about the outcome
   of a random process, not physical disorder. The ticket-box game
   makes this precise: entropy is the average number of questions
   needed. Probe: "What does it mean for Box 1 to have entropy zero?
   Is it 'ordered' in some physical sense?"

2. **KL divergence is a distance.** KL divergence is not symmetric:
   KL(P || Q) ≠ KL(Q || P). It also does not satisfy the triangle
   inequality. Therefore it is not a distance metric. Probe: "If
   KL(P || Q) measures the cost of using Q when P is true, what
   does KL(Q || P) measure? Are these the same question?"

3. **Mutual information and correlation measure the same thing.**
   Correlation captures only linear association. MI captures any
   statistical dependence. For Y = X² with symmetric X, correlation
   is zero but MI is positive. Probe: "Can you construct an example
   where two variables are strongly related but have zero correlation?"

4. **Entropy is only relevant to information theory or physics.**
   Entropy appears directly in decision tree construction (information
   gain), neural network training (cross-entropy loss), and model
   comparison (KL divergence). Probe: "When a decision tree picks a
   feature to split on, what quantity is it maximizing? How does that
   connect to entropy?"

5. **The base of the logarithm matters fundamentally.** Changing from
   log₂ to log_e changes H by a multiplicative constant. The
   conceptual content is unchanged; only the units change (bits vs.
   nats). Probe: "If I define H using natural log instead of log₂,
   does the ranking of distributions by entropy change?"

---

## Scaffolding Strategies

### When a student is stuck on entropy calculations

- "Start with the ticket-box examples. For Box 4 ({A, A, B, C}),
  what is the probability of each letter? Plug into the formula."
- "Entropy is a weighted average. Each term p × log₂(1/p) is the
  'surprise' of outcome k, weighted by its probability. Outcomes
  that are unlikely contribute more surprise when they occur."
- For math students: "Verify that H is maximized when the
  distribution is uniform. Take the derivative and set it to zero."

### When a student is stuck on mutual information

- "Start from the definition: MI = H(X) + H(Y) - H(X,Y). What
  does each term measure?"
- "If X and Y are independent, what is H(X,Y)? What does that
  make MI?"
- Use the Venn diagram: "H(X) and H(Y) overlap. The overlap is MI.
  What does the non-overlapping part represent?"

### When a student is stuck on KL divergence

- "Think of it as the penalty for using the wrong codebook. You
  designed your questions for distribution Q, but the actual
  distribution is P. How many extra questions do you need?"
- "Work through the chapter's example with the two ticket boxes.
  Calculate each term of the sum."

### When a student is stuck on Exercise 2 (UCB Admissions entropy)

- "First, compute the marginal distribution of decisions (Admitted
  vs. Rejected). What is H_decision?"
- "Now compute the joint distribution of (decision, sex). What is
  H_decision,sex? What is MI?"
- Connect to Chapter 2: "You saw Simpson's paradox in this data.
  How does adding department information change the mutual
  information between sex and decision?"

### When a student is stuck on Exercise 3 (Cross-entropy intuition)

- "For each predicted distribution, compute -log₂(predicted
  probability of the true class). That is the cross-entropy loss
  for that observation."
- "What happens as the predicted probability for the true class
  approaches zero? What does that mean for the loss?"
- "Why is (0.9, 0.05, 0.05) a better prediction than (0.33, 0.33,
  0.34) for a class-1 observation?"

### When a student is stuck on Exercise 4 (MI vs. correlation)

- "Generate data where Y = X² and X ~ N(0,1). What is the
  correlation? Now discretize X and Y into bins and compute MI.
  What do you find?"
- "When would you use MI instead of correlation for feature
  selection?"

---

## Key Connections

**Backward to Chapters 1-5:**

- The chi-squared test (Chapter 2) tests independence of categorical
  variables. MI measures the *degree* of dependence, providing a
  continuous alternative to the binary hypothesis test.
- Simpson's paradox (Chapter 2) and the UCB admissions data return
  in Exercise 2, now analyzed through an information-theoretic lens.
- Correlation (Chapter 2) is compared to MI: both measure association,
  but MI captures nonlinear relationships.
- The standard error (Chapter 5) quantifies uncertainty about the
  mean; entropy quantifies uncertainty about a discrete outcome.

**Forward references:**

- Decision tree algorithms (encountered in later ML courses) use
  information gain to select splitting features.
- Cross-entropy loss is the standard objective for training
  classification models (logistic regression, neural networks).
- KL divergence appears in variational inference (VAEs) and in
  comparing topic model distributions (Chapter 11).
- MI-based feature selection provides a principled alternative to
  correlation-based approaches.

**If the student asks about topics not yet covered:**

If a student asks about decision trees, cross-entropy in neural
networks, or variational inference, provide the conceptual connection
(the chapter describes these in the "Where These Ideas Appear in ML"
section) without developing the supervised learning machinery. The
chapter is designed to equip the student with vocabulary, not to
teach the algorithms.

---

## Terminology

| Term                    | Definition (per the book)                                                                       |
| ----------------------- | ----------------------------------------------------------------------------------------------- |
| Entropy (H)             | A measure of uncertainty: average number of yes-no questions to identify a randomly drawn value |
| Bit                     | Unit of entropy when using log base 2                                                           |
| Nat                     | Unit of entropy when using natural logarithm                                                    |
| Joint entropy           | H(X, Y): uncertainty about the pair (X, Y)                                                      |
| Conditional entropy     | H(X\|Y): remaining uncertainty about X after observing Y                                        |
| Mutual information (MI) | H(X) + H(Y) - H(X, Y): reduction in uncertainty about one variable from knowing the other       |
| Information gain        | Synonym for mutual information; used especially in decision tree context                        |
| KL divergence           | KL(P \|\| Q): additional cost of using distribution Q when the truth is P                       |
| Cross-entropy           | H(P, Q) = H(P) + KL(P \|\| Q): total average code length using Q for data from P                |
| Cross-entropy loss      | The standard loss function for classification; equivalent to minimizing KL divergence           |

---

## Companion Guidance for This Chapter

When helping a student with Chapter 6 material:

- **The ticket-box game is the anchor.** If a student is struggling
  with any definition, return to the game. Entropy is the average
  number of questions. MI is the reduction in questions when
  variables are dependent. KL divergence is the extra questions
  from using the wrong strategy. Every formula has a concrete
  interpretation in terms of the game.

- **This chapter is about vocabulary, not algorithms.** The student
  is not expected to implement a decision tree or train a neural
  network. They are expected to understand what entropy, MI, KL
  divergence, and cross-entropy *mean* so they can recognize these
  concepts when they encounter them later.

- **Exercise 2 connects to Chapter 2 beautifully.** The UCB
  admissions data appeared in the Simpson's paradox discussion.
  Now the student analyzes the same data through an information-
  theoretic lens. Help them see the connection: MI between sex and
  admission decision measures how much sex tells you about admission,
  and conditioning on department changes that relationship.

- **Exercise 3 builds loss function intuition.** Even though the
  student will not train a classifier in this chapter, understanding
  why cross-entropy loss penalizes confident wrong predictions more
  heavily than uncertain ones is foundational. The limiting case
  (predicted probability → 0 for the true class, loss → ∞) is
  important.

- **Exercise 4 bridges correlation and MI.** This is the exercise
  where the student sees concretely that MI captures structure
  that correlation misses. If the student constructed Y = X²
  examples in Chapter 2 Exercise 4, connect back to that.

- **Respect the coda character.** Some students will find this
  chapter disconnected from the EDA narrative. That feeling is
  legitimate. Acknowledge it: "This chapter is planting seeds
  that will sprout when you study decision trees, neural networks,
  and Bayesian methods. For now, focus on the definitions and
  the game-based intuition."

- **Celebrate mathematical exploration.** The proof that H(X, Y) =
  H(X) + H(Y) for independent variables is a clean algebraic
  exercise. A student who works through it, or who explores
  properties of entropy (concavity, maximization), is doing
  productive mathematical reasoning.
