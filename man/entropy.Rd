% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/information_theory.R
\name{entropy}
\alias{entropy}
\title{Calculate entropy of a discrete probability distribution}
\usage{
entropy(p, base = 2)
}
\arguments{
\item{p}{Numeric vector of probabilities. Will be normalized to sum to 1
if it does not already. Zero and negative values are removed before
calculation.}

\item{base}{Numeric scalar: logarithm base. Default 2 gives entropy in
bits; use \code{exp(1)} for nats, or 10 for hartleys.}
}
\value{
Numeric scalar: entropy value. Units depend on \code{base}:
  bits (base 2), nats (base e), or hartleys (base 10).
}
\description{
Computes Shannon entropy for a finite probability distribution, measuring
the average uncertainty (in bits) about the outcome of a random variable.
}
\details{
Shannon entropy is defined as:
\deqn{H(X) = -\sum_{i=1}^{K} p_i \log_b(p_i)}
where \eqn{p_i} is the probability of outcome \eqn{i} and \eqn{b} is the
logarithm base.

Key properties:
\itemize{
  \item \eqn{H \geq 0} always (non-negative)
  \item \eqn{H = 0} if and only if one outcome has probability 1
        (complete certainty)
  \item \eqn{H} is maximized when all outcomes are equally likely
        (uniform distribution)
  \item For \eqn{K} equally likely outcomes: \eqn{H = \log_b(K)}
}

The convention \eqn{0 \log(0) = 0} is used, consistent with the limit
\eqn{\lim_{p \to 0^+} p \log(p) = 0}.
}
\section{Interpretation}{

Entropy can be understood as the average number of yes/no questions
needed to identify an outcome drawn from the distribution, assuming
an optimal questioning strategy.
}

\examples{
# Fair coin: maximum entropy for 2 outcomes = 1 bit
entropy(c(0.5, 0.5))

# Biased coin: lower entropy (more predictable)
entropy(c(0.9, 0.1))
entropy(c(0.99, 0.01))

# Fair 6-sided die: log2(6) approx 2.58 bits
entropy(rep(1/6, 6))

# Degenerate distribution: 0 bits (no uncertainty)
entropy(c(1, 0, 0, 0))

# Unnormalized input is automatically normalized
entropy(c(1, 1, 1, 1))
# same as
entropy(c(0.25, 0.25, 0.25, 0.25))

# Using natural log (nats) instead of bits
entropy(c(0.5, 0.5), base = exp(1))

}
\references{
Shannon, C. E. (1948). A Mathematical Theory of Communication.
\emph{Bell System Technical Journal}, 27(3), 379-423.
}
\seealso{
\code{\link{estimate_entropy_from_sample}} for estimating entropy from data,
\code{\link{kl_divergence}} for comparing two distributions,
\code{\link{information_gain}} for decision tree splitting
}
