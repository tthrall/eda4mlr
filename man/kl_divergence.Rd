% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/information_theory.R
\name{kl_divergence}
\alias{kl_divergence}
\title{Calculate KL divergence between two distributions}
\usage{
kl_divergence(p, q, base = 2)
}
\arguments{
\item{p}{Numeric vector: reference (true) distribution. Can be a named
vector for alignment with \code{q}.}

\item{q}{Numeric vector: alternative (model/assumed) distribution. Can be
a named vector.}

\item{base}{Numeric scalar: logarithm base. Default 2 for bits.}
}
\value{
Numeric scalar: KL divergence value. Returns \code{Inf} if
  \code{q} assigns zero probability to any outcome where \code{p} is
  positive (the distributions have incompatible support).
}
\description{
Computes Kullback-Leibler divergence \eqn{D_{KL}(P \| Q)}, measuring the
expected number of extra bits needed when using a code optimized for
distribution \eqn{Q} but the true distribution is \eqn{P}.
}
\details{
KL divergence is defined as:
\deqn{D_{KL}(P \| Q) = \sum_{x: P(x) > 0} P(x) \log_b\left(\frac{P(x)}{Q(x)}\right)}

Key properties:
\itemize{
  \item \eqn{D_{KL}(P \| Q) \geq 0} always (Gibbs inequality)
  \item \eqn{D_{KL}(P \| Q) = 0} if and only if \eqn{P = Q}
  \item Not symmetric: \eqn{D_{KL}(P \| Q) \neq D_{KL}(Q \| P)} in general
  \item Not a metric: does not satisfy the triangle inequality
}
}
\section{Interpretation}{

KL divergence quantifies the cost of being wrong about a distribution.
If you design a coding scheme assuming distribution \eqn{Q}, but the true
distribution is \eqn{P}, you will use \eqn{D_{KL}(P \| Q)} extra bits per
symbol on average.
}

\section{Asymmetry}{

The asymmetry of KL divergence has important practical implications:
\itemize{
  \item \eqn{D_{KL}(P \| Q)}: cost of using \eqn{Q} when truth is \eqn{P}
  \item \eqn{D_{KL}(Q \| P)}: cost of using \eqn{P} when truth is \eqn{Q}
}
These can differ substantially. In variational inference, minimizing
\eqn{D_{KL}(Q \| P)} (reverse KL) yields mode-seeking behavior, while
minimizing \eqn{D_{KL}(P \| Q)} (forward KL) yields mean-seeking behavior.
}

\section{Named vectors}{

If both \code{p} and \code{q} are named vectors, alignment is performed
by name (only shared names are used). If unnamed, positional alignment
is assumed and lengths must match.
}

\examples{
# Identical distributions: KL = 0
p <- c(0.5, 0.5)
kl_divergence(p, p)

# Uniform vs. biased: asymmetric!
q <- c(0.9, 0.1)
kl_divergence(p, q)
kl_divergence(q, p)

# Named vectors with automatic alignment
p_named <- c(A = 0.5, B = 0.3, C = 0.2)
q_named <- c(B = 0.4, C = 0.3, A = 0.3)
kl_divergence(p_named, q_named)

# Infinite divergence when supports do not match
p_sparse <- c(0.5, 0.5, 0)
q_sparse <- c(0.33, 0.33, 0.34)
kl_divergence(q_sparse, p_sparse)

}
\references{
Kullback, S., and Leibler, R. A. (1951). On Information and Sufficiency.
Annals of Mathematical Statistics, 22(1), 79-86.
}
\seealso{
\code{\link{entropy}} for single-distribution uncertainty,
\code{\link{fdiv_kernel_list}} for other f-divergences
}
