% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/information_theory.R
\name{estimate_entropy_from_sample}
\alias{estimate_entropy_from_sample}
\title{Estimate entropy from a sample}
\usage{
estimate_entropy_from_sample(sample, possible_outcomes = NULL, base = 2)
}
\arguments{
\item{sample}{Vector of observed values (any type that can be tabulated).}

\item{possible_outcomes}{Vector of all possible outcome values. If
\code{NULL} (default), uses the unique values observed in \code{sample}.
Specifying this parameter is important when some outcomes may have
zero observations but are theoretically possible.}

\item{base}{Numeric scalar: logarithm base. Default 2 for bits.}
}
\value{
Numeric scalar: estimated entropy in bits (or other units if
  \code{base} is changed).
}
\description{
Computes a plug-in (maximum likelihood) entropy estimate from observed
frequencies in a sample.
}
\details{
The plug-in estimator computes entropy from the empirical distribution:
\deqn{\hat{H} = -\sum_{i=1}^{K} \hat{p}_i \log_b(\hat{p}_i)}
where \eqn{\hat{p}_i = n_i / n} is the observed frequency of outcome \eqn{i}.
}
\section{Bias and sample size}{

The plug-in estimator is negatively biased: it systematically
underestimates true entropy, especially when:
\itemize{
  \item Sample size \eqn{n} is small relative to the number of outcomes \eqn{K}
  \item The true distribution has many low-probability outcomes
}

As a rule of thumb, reliable estimates require \eqn{n \gg K}. For small
samples, consider bias-corrected estimators (Miller-Madow, jackknife) or
Bayesian approaches, which are not implemented here.
}

\section{Specifying possible outcomes}{

When \code{possible_outcomes} is \code{NULL}, only observed values
contribute to the estimate. This can inflate entropy estimates if the
true distribution includes outcomes that happen to be unobserved in the
sample. Specifying \code{possible_outcomes} explicitly ensures that
unobserved-but-possible outcomes contribute zero to the sum (via the
\eqn{0 \log(0) = 0} convention).
}

\examples{
# Estimate entropy of a fair die from 100 rolls
set.seed(42)
rolls <- sample(1:6, 100, replace = TRUE)
estimate_entropy_from_sample(rolls, possible_outcomes = 1:6)

# Compare to true entropy
entropy(rep(1/6, 6))

# Without specifying possible_outcomes (uses only observed values)
set.seed(99)
small_sample <- sample(1:6, 10, replace = TRUE)
estimate_entropy_from_sample(small_sample)

}
\seealso{
\code{\link{entropy}} for computing entropy from a known distribution
}
