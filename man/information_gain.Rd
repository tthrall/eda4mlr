% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/information_theory.R
\name{information_gain}
\alias{information_gain}
\title{Calculate information gain from a feature split}
\usage{
information_gain(data, target, feature, base = 2)
}
\arguments{
\item{data}{Data frame containing the target and feature variables.}

\item{target}{Character string: name of the target variable (column in
\code{data}). Typically a factor or character column.}

\item{feature}{Character string: name of the feature to evaluate for
splitting (column in \code{data}).}

\item{base}{Numeric scalar: logarithm base. Default 2 for bits.}
}
\value{
Numeric scalar: information gain in bits (or other units). Higher
  values indicate more informative splits.
}
\description{
Computes the reduction in entropy achieved by partitioning data on a
categorical feature. This is the criterion used by ID3 and C4.5 decision
tree algorithms to select splitting features.
}
\details{
Information gain is defined as:
\deqn{IG(T, F) = H(T) - H(T | F) = H(T) - \sum_{v \in F} \frac{|T_v|}{|T|} H(T_v)}
where:
\itemize{
  \item \eqn{H(T)} is the entropy of the target variable
  \item \eqn{T_v} is the subset of data where feature \eqn{F} has value \eqn{v}
  \item \eqn{H(T_v)} is the entropy of the target within subset \eqn{T_v}
}
}
\section{Interpretation}{

Information gain measures how much knowing the feature value reduces
uncertainty about the target. A split with high information gain
creates child nodes that are more pure (lower entropy) than the parent.
}

\section{Limitations}{

Information gain is biased toward features with many distinct values.
A feature like customer_id (unique per row) achieves perfect
information gain but has no predictive value for new data. Decision
tree implementations often use gain ratio (information gain
divided by the features intrinsic entropy) to correct for this bias.
}

\examples{
# Simple example: contract type predicts churn
churn_data <- data.frame(
  contract = c(rep("monthly", 10), rep("annual", 10)),
  churned = c(rep("yes", 8), rep("no", 2), rep("yes", 2), rep("no", 8))
)
information_gain(churn_data, "churned", "contract")

# Pathological case: unique ID achieves maximum IG but overfits
churn_data$customer_id <- seq_len(nrow(churn_data))
information_gain(churn_data, "churned", "customer_id")

}
\references{
Quinlan, J. R. (1986). Induction of Decision Trees.
Machine Learning, 1(1), 81-106.
}
\seealso{
\code{\link{entropy}} for the underlying entropy calculation
}
